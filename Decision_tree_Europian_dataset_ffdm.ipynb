{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqWaU_9YZZda",
        "outputId": "c849a193-96f7-4d36-d11e-bce21c33d25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlzV9w5EWalk",
        "outputId": "90da7d7c-387a-4f74-8a8f-d6a87fac3d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in 'Class': 1\n",
            "Dataset shape after dropping NaN in 'Class': (15935, 31)\n",
            "Train shape: (12748, 31), Test shape: (3187, 31)\n",
            "Best Parameters: {'ccp_alpha': 0.01, 'class_weight': 'balanced', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
            "Optimal Threshold: 0.9041\n",
            "\n",
            "Train Metrics:\n",
            "Train Precision: 0.9884\n",
            "Train Recall: 0.9894\n",
            "Train F1: 0.9889\n",
            "Train AUC: 0.9932\n",
            "Train Accuracy: 0.9926\n",
            "\n",
            "Test (Validation) Metrics:\n",
            "Test Precision: 0.9898\n",
            "Test Recall: 0.9929\n",
            "Test F1: 0.9913\n",
            "Test AUC: 0.9949\n",
            "Test Accuracy: 0.9942\n",
            "\n",
            "Feature Importance:\n",
            "              Feature  Importance\n",
            "13                V12    0.953862\n",
            "5                  V4    0.024124\n",
            "15                V14    0.022014\n",
            "2                  V1    0.000000\n",
            "3                  V2    0.000000\n",
            "4                  V3    0.000000\n",
            "1   LogTransactionAmt    0.000000\n",
            "6                  V5    0.000000\n",
            "7                  V6    0.000000\n",
            "9                  V8    0.000000\n",
            "8                  V7    0.000000\n",
            "10                 V9    0.000000\n",
            "11                V10    0.000000\n",
            "12                V11    0.000000\n",
            "0              Amount    0.000000\n",
            "14                V13    0.000000\n",
            "16                V15    0.000000\n",
            "17                V16    0.000000\n",
            "18                V17    0.000000\n",
            "19                V18    0.000000\n",
            "20                V19    0.000000\n",
            "21                V20    0.000000\n",
            "22                V21    0.000000\n",
            "23                V22    0.000000\n",
            "24                V23    0.000000\n",
            "25                V24    0.000000\n",
            "26                V25    0.000000\n",
            "27                V26    0.000000\n",
            "28                V27    0.000000\n",
            "29                V28    0.000000\n",
            "30       hour_warning    0.000000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 1: Load the Data\n",
        "# Assume a single CSV file for the credit card dataset\n",
        "data = pd.read_csv(r\"/content/creditcard.csv\")  # Replace with your dataset path\n",
        "\n",
        "# Check for NaN in 'Class' and handle them\n",
        "print(f\"Number of NaN values in 'Class': {data['Class'].isna().sum()}\")\n",
        "data = data.dropna(subset=['Class'])  # Drop rows where 'Class' is NaN\n",
        "data['Class'] = data['Class'].astype(int)  # Ensure 'Class' is integer (0 or 1)\n",
        "print(f\"Dataset shape after dropping NaN in 'Class': {data.shape}\")\n",
        "\n",
        "# Split into train and test (80-20 split)\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['Class'])\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "# Clean up memory\n",
        "del data\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "# Remove features with only one unique value\n",
        "one_unique_col = [col for col in train.columns if train[col].nunique() == 1]\n",
        "train.drop(columns=one_unique_col, inplace=True)\n",
        "test.drop(columns=one_unique_col, inplace=True)\n",
        "\n",
        "# Define categorical columns (adjust based on dataset)\n",
        "base_categorical_cols = ['card_type', 'email_domain'] if 'card_type' in train.columns else []\n",
        "m_cols = [f'M{i}' for i in range(1, 10) if f'M{i}' in train.columns]\n",
        "categorical_cols = base_categorical_cols + m_cols\n",
        "categorical_cols = [col for col in categorical_cols if col in train.columns]\n",
        "\n",
        "# Handle missing values\n",
        "numerical_cols = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "numerical_cols = [col for col in numerical_cols if col not in ['Class', 'TransactionID']]\n",
        "for col in numerical_cols:\n",
        "    train[col].fillna(train[col].median(), inplace=True)\n",
        "    test[col].fillna(test[col].median(), inplace=True)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    train[col] = train[col].fillna('missing').astype(str)\n",
        "    test[col] = test[col].fillna('missing').astype(str)\n",
        "\n",
        "# Remove outliers (adjust based on dataset)\n",
        "if 'Amount' in train.columns:\n",
        "    train.drop(train[train['Amount'] > 10000].index, inplace=True)\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# Time-based features\n",
        "if 'Time' in train.columns:\n",
        "    train['day'] = ((train['Time'] // (3600 * 24) - 1) % 7) + 1\n",
        "    test['day'] = ((test['Time'] // (3600 * 24) - 1) % 7) + 1\n",
        "    train['hour'] = ((train['Time'] // 3600) % 24) + 1\n",
        "    test['hour'] = ((test['Time'] // 3600) % 24) + 1\n",
        "\n",
        "def new_hr_feature(hr):\n",
        "    if hr >= 7 and hr < 10:\n",
        "        return \"highwarningsign\"\n",
        "    elif hr >= 14 and hr < 16:\n",
        "        return \"lowestwarningsign\"\n",
        "    elif (hr >= 4 and hr < 7) or (hr >= 10 and hr < 14):\n",
        "        return \"mediumwarningsign\"\n",
        "    else:\n",
        "        return \"lowwarningsign\"\n",
        "\n",
        "if 'hour' in train.columns:\n",
        "    train['hour_warning'] = train['hour'].apply(new_hr_feature)\n",
        "    test['hour_warning'] = test['hour'].apply(new_hr_feature)\n",
        "\n",
        "# Log transform Amount\n",
        "if 'Amount' in train.columns:\n",
        "    train['LogTransactionAmt'] = np.log(train['Amount'] + 1e-9)\n",
        "    test['LogTransactionAmt'] = np.log(test['Amount'] + 1e-9)\n",
        "\n",
        "# New feature for card3 (if exists)\n",
        "if 'card3' in train.columns:\n",
        "    def new_card3(row):\n",
        "        try:\n",
        "            val = float(row)\n",
        "            return 'Positive' if val > 160 else 'Negative'\n",
        "        except:\n",
        "            return 'missing'\n",
        "    train['new_card3'] = train['card3'].apply(new_card3)\n",
        "    test['new_card3'] = test['card3'].apply(new_card3)\n",
        "\n",
        "# Simplify card6 (if exists)\n",
        "if 'card6' in train.columns:\n",
        "    def replacetodebit(row):\n",
        "        if row in ['debit or credit', 'charge card']:\n",
        "            return 'debit'\n",
        "        return row\n",
        "    train['card6'] = train['card6'].apply(replacetodebit)\n",
        "    test['card6'] = test['card6'].apply(replacetodebit)\n",
        "\n",
        "# Group email domains (if exists)\n",
        "if 'P_emaildomain' in train.columns:\n",
        "    for dataset in [train, test]:\n",
        "        dataset.loc[dataset['P_emaildomain'].isin(['gmail.com', 'gmail']), 'P_emaildomain'] = 'Google'\n",
        "        dataset.loc[dataset['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx', 'yahoo.co.uk',\n",
        "                                                  'yahoo.co.jp', 'yahoo.de', 'yahoo.fr', 'yahoo.es']),\n",
        "                    'P_emaildomain'] = 'Yahoo'\n",
        "        dataset.loc[dataset['P_emaildomain'].isin(['hotmail.com', 'outlook.com', 'msn.com',\n",
        "                                                  'live.com.mx', 'hotmail.es', 'hotmail.co.uk',\n",
        "                                                  'hotmail.de', 'outlook.es', 'live.com',\n",
        "                                                  'live.fr', 'hotmail.fr']),\n",
        "                    'P_emaildomain'] = 'Microsoft'\n",
        "        dataset.loc[dataset['P_emaildomain'].isin(dataset['P_emaildomain'].value_counts()[\n",
        "                    dataset['P_emaildomain'].value_counts() <= 500].index),\n",
        "                    'P_emaildomain'] = 'Others'\n",
        "        dataset['P_emaildomain'].fillna('NoInf', inplace=True)\n",
        "        if 'R_emaildomain' in dataset.columns:\n",
        "            dataset.loc[dataset['R_emaildomain'].isin(['gmail.com', 'gmail']), 'R_emaildomain'] = 'Google'\n",
        "            dataset.loc[dataset['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx', 'yahoo.co.uk',\n",
        "                                                      'yahoo.co.jp', 'yahoo.de', 'yahoo.fr', 'yahoo.es']),\n",
        "                        'R_emaildomain'] = 'Yahoo'\n",
        "            dataset.loc[dataset['R_emaildomain'].isin(['hotmail.com', 'outlook.com', 'msn.com',\n",
        "                                                      'live.com.mx', 'hotmail.es', 'hotmail.co.uk',\n",
        "                                                      'hotmail.de', 'outlook.es', 'live.com',\n",
        "                                                      'live.fr', 'hotmail.fr']),\n",
        "                        'R_emaildomain'] = 'Microsoft'\n",
        "            dataset.loc[dataset['R_emaildomain'].isin(dataset['R_emaildomain'].value_counts()[\n",
        "                        dataset['R_emaildomain'].value_counts() <= 300].index),\n",
        "                        'R_emaildomain'] = 'Others'\n",
        "            dataset['R_emaildomain'].fillna('NoInf', inplace=True)\n",
        "\n",
        "# Transaction frequency\n",
        "if 'card1' in train.columns:\n",
        "    train['card1_freq'] = train.groupby('card1')['card1'].transform('count')\n",
        "    test['card1_freq'] = test.groupby('card1')['card1'].transform('count')\n",
        "\n",
        "if 'addr1' in train.columns:\n",
        "    train['addr1_freq'] = train.groupby('addr1')['addr1'].transform('count')\n",
        "    test['addr1_freq'] = test.groupby('addr1')['addr1'].transform('count')\n",
        "\n",
        "# Interaction feature\n",
        "if 'card1_freq' in train.columns and 'Amount' in train.columns:\n",
        "    train['amt_per_card1'] = train['Amount'] / (train['card1_freq'] + 1e-9)\n",
        "    test['amt_per_card1'] = test['Amount'] / (test['card1_freq'] + 1e-9)\n",
        "\n",
        "# Step 4: Feature Preparation\n",
        "# Select top features\n",
        "numerical_cols = ['Amount', 'LogTransactionAmt', 'card1_freq', 'addr1_freq', 'amt_per_card1'] if 'card1_freq' in train.columns else ['Amount', 'LogTransactionAmt']\n",
        "numerical_cols += [col for col in train.columns if col.startswith('V')]\n",
        "numerical_cols = [col for col in numerical_cols if col in train.columns]\n",
        "\n",
        "categorical_cols = ['ProductCD', 'card6', 'new_card3', 'P_emaildomain', 'R_emaildomain', 'hour_warning']\n",
        "categorical_cols = [col for col in categorical_cols if col in train.columns]\n",
        "\n",
        "# Handle categorical encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = train[col].astype(str)\n",
        "    train_unique_values = train[col].unique()\n",
        "    le.fit(train_unique_values)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = test[col].astype(str)\n",
        "    test[col] = test[col].apply(lambda x: x if x in train_unique_values else 'unknown')\n",
        "    if 'unknown' not in le.classes_:\n",
        "        le_classes = list(le.classes_)\n",
        "        le_classes.append('unknown')\n",
        "        le.classes_ = np.array(le_classes)\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Combine features\n",
        "features = numerical_cols + categorical_cols\n",
        "X = train[features]\n",
        "y = train['Class']\n",
        "X_test = test[features]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Handle Class Imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
        "X_train, y_train = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# Step 6: Hyperparameter Tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [5, 7, 9],\n",
        "    'min_samples_split': [5, 10, 20],\n",
        "    'min_samples_leaf': [2, 5],\n",
        "    'class_weight': ['balanced'],\n",
        "    'ccp_alpha': [0.01, 0.05, 0.1]\n",
        "}\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_dt = grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Step 7: Adaptive Thresholding\n",
        "val_probs = best_dt.predict_proba(X_val)[:, 1]\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, val_probs)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)\n",
        "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate Model\n",
        "train_probs = best_dt.predict_proba(X_train)[:, 1]\n",
        "val_probs = best_dt.predict_proba(X_val)[:, 1]\n",
        "\n",
        "train_pred = (train_probs >= optimal_threshold).astype(int)\n",
        "val_pred = (val_probs >= optimal_threshold).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1 = f1_score(y_train, train_pred)\n",
        "train_auc = roc_auc_score(y_train, train_probs)\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "\n",
        "val_precision = precision_score(y_val, val_pred)\n",
        "val_recall = recall_score(y_val, val_pred)\n",
        "val_f1 = f1_score(y_val, val_pred)\n",
        "val_auc = roc_auc_score(y_val, val_probs)\n",
        "val_accuracy = accuracy_score(y_val, val_pred)\n",
        "\n",
        "# Print train and validation metrics\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Train F1: {train_f1:.4f}\")\n",
        "print(f\"Train AUC: {train_auc:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTest (Validation) Metrics:\")\n",
        "print(f\"Test Precision: {val_precision:.4f}\")\n",
        "print(f\"Test Recall: {val_recall:.4f}\")\n",
        "print(f\"Test F1: {val_f1:.4f}\")\n",
        "print(f\"Test AUC: {val_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': best_dt.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AV4E2y2Wl4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}