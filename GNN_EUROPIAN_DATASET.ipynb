{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbNpTyqVCwum",
        "outputId": "3ad093b1-1d7d-425a-fae7-8163472ceb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvaBOQdnCcrJ",
        "outputId": "d952d9ba-fecd-4595-e116-13ae509fe79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (11959, 31)\n",
            "Missing values in each column:\n",
            "V20       0.000084\n",
            "V21       0.000084\n",
            "V22       0.000084\n",
            "V23       0.000084\n",
            "V24       0.000084\n",
            "V25       0.000084\n",
            "V26       0.000084\n",
            "V27       0.000084\n",
            "V28       0.000084\n",
            "Amount    0.000084\n",
            "Class     0.000084\n",
            "dtype: float64\n",
            "Found 1 NaN values in 'Class' column.\n",
            "After dropping NaN in 'Class', data shape: (11958, 31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3c34b5d2d64b>:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[col] = data[col].fillna(data[col].median())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7063\n",
            "Epoch 10, Loss: 0.1501\n",
            "Epoch 20, Loss: 0.0733\n",
            "Epoch 30, Loss: 0.0472\n",
            "Epoch 40, Loss: 0.0392\n",
            "Epoch 50, Loss: 0.0299\n",
            "Epoch 60, Loss: 0.0239\n",
            "Epoch 70, Loss: 0.0215\n",
            "Epoch 80, Loss: 0.0181\n",
            "Epoch 90, Loss: 0.0187\n",
            "\n",
            "Test Set Performance:\n",
            "Test Precision: 0.9957\n",
            "Test Recall: 0.9979\n",
            "Test F1: 0.9968\n",
            "Test AUC: 0.9989\n",
            "Test Accuracy: 0.9968\n",
            "\n",
            "Train Set Performance:\n",
            "Train Precision: 0.9962\n",
            "Train Recall: 0.9980\n",
            "Train F1: 0.9971\n",
            "Train AUC: 0.9997\n",
            "Train Accuracy: 0.9971\n",
            "Submission file generated: submission.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Step 1: Load the Data\n",
        "data = pd.read_csv('/content/creditcard.csv')\n",
        "\n",
        "# Clean up memory\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "# Step 2: Enhanced Preprocessing\n",
        "# 2.1 Check for missing values\n",
        "missing_percent = data.isnull().mean()\n",
        "print(f\"Missing values in each column:\\n{missing_percent[missing_percent > 0]}\")\n",
        "\n",
        "# 2.2 Check for NaN in 'Class' column specifically\n",
        "if data['Class'].isna().sum() > 0:\n",
        "    print(f\"Found {data['Class'].isna().sum()} NaN values in 'Class' column.\")\n",
        "    # Option 1: Drop rows with NaN in 'Class'\n",
        "    data = data.dropna(subset=['Class'])\n",
        "    print(f\"After dropping NaN in 'Class', data shape: {data.shape}\")\n",
        "    # Option 2: (Alternative) Impute with mode (0 or 1, likely 0 for non-fraud)\n",
        "    # data['Class'] = data['Class'].fillna(data['Class'].mode()[0])\n",
        "\n",
        "# 2.3 Define feature columns\n",
        "numerical_cols = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "target_col = 'Class'\n",
        "\n",
        "# 2.4 Handle missing values in numerical columns\n",
        "for col in numerical_cols:\n",
        "    data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "# 2.5 Split data into train and test sets\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['Class'])\n",
        "\n",
        "# 2.6 Remove outliers using IQR for 'Amount' (only for training data)\n",
        "for col in ['Amount']:\n",
        "    q1 = train[col].quantile(0.05)\n",
        "    q3 = train[col].quantile(0.95)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]\n",
        "\n",
        "# Step 3: Enhanced Feature Engineering\n",
        "# 3.1 Time-based features\n",
        "train['hour'] = (train['Time'] // 3600) % 24\n",
        "test['hour'] = (test['Time'] // 3600) % 24\n",
        "\n",
        "# 3.2 Log transform Amount\n",
        "train['LogAmount'] = np.log1p(train['Amount'])\n",
        "test['LogAmount'] = np.log1p(test['Amount'])\n",
        "\n",
        "# 3.3 Transaction frequency features (based on 'Time' proximity)\n",
        "train['time_freq'] = train.groupby('Time')['Time'].transform('count')\n",
        "test['time_freq'] = test.groupby('Time')['Time'].transform('count')\n",
        "train['time_freq'] = train['time_freq'].fillna(1)\n",
        "test['time_freq'] = test['time_freq'].fillna(1)\n",
        "\n",
        "# Step 4: Prepare Data for GNN\n",
        "# 4.1 Define feature set\n",
        "numerical_cols = ['LogAmount', 'hour', 'time_freq'] + [f'V{i}' for i in range(1, 29)]\n",
        "features = numerical_cols\n",
        "\n",
        "X = train[features]\n",
        "y = train['Class']\n",
        "X_test_full = test[features]\n",
        "\n",
        "# 4.2 Scale features (StandardScaler first, then MinMaxScaler to [0, 0.7])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 0.7))\n",
        "X_scaled = minmax_scaler.fit_transform(X_scaled)\n",
        "X_test_scaled = minmax_scaler.transform(X_test_scaled)\n",
        "\n",
        "# 4.3 Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# 4.4 Create graph structure (edges based on similarity in features)\n",
        "def create_edge_index(X, k=5):\n",
        "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)\n",
        "    distances, indices = nbrs.kneighbors(X)\n",
        "    edge_index = []\n",
        "    for i in range(len(X)):\n",
        "        for j in indices[i][1:]:  # Skip self (first neighbor)\n",
        "            edge_index.append([i, j])\n",
        "            edge_index.append([j, i])  # Undirected graph\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "# Create edge_index for training and test data\n",
        "train_edge_index = create_edge_index(X_resampled)\n",
        "test_edge_index = create_edge_index(X_test_scaled)\n",
        "\n",
        "# 4.5 Create PyTorch Geometric Data object for training\n",
        "x = torch.tensor(X_resampled, dtype=torch.float)\n",
        "y = torch.tensor(y_resampled.values, dtype=torch.long)\n",
        "data = Data(x=x, edge_index=train_edge_index, y=y)\n",
        "\n",
        "# 4.6 Create train/val/test masks\n",
        "n_samples = len(y_resampled)\n",
        "train_idx, temp_idx = train_test_split(range(n_samples), test_size=0.3, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "train_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# 4.7 Create PyTorch Geometric Data object for test set\n",
        "test_data = Data(x=torch.tensor(X_test_scaled, dtype=torch.float), edge_index=test_edge_index)\n",
        "\n",
        "# Step 5: Define GNN Model\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 6: Train GNN Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNN(input_dim=X_resampled.shape[1], hidden_dim=64, output_dim=2).to(device)\n",
        "data = data.to(device)\n",
        "test_data = test_data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        probs = torch.softmax(out[mask], dim=1)[:, 1].cpu().numpy()\n",
        "        pred = (probs >= optimal_threshold).astype(int)\n",
        "        true = data.y[mask].cpu().numpy()\n",
        "        precision = precision_score(true, pred)\n",
        "        recall = recall_score(true, pred)\n",
        "        f1 = f1_score(true, pred)\n",
        "        auc = roc_auc_score(true, probs)\n",
        "        accuracy = accuracy_score(true, pred)\n",
        "    return precision, recall, f1, auc, accuracy\n",
        "\n",
        "# Find optimal threshold on validation set\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    val_probs = torch.softmax(out[data.val_mask], dim=1)[:, 1].cpu().numpy()\n",
        "    val_true = data.y[data.val_mask].cpu().numpy()\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(val_true, val_probs)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "# Step 7: Evaluate on Test Set\n",
        "test_precision, test_recall, test_f1, test_auc, test_accuracy = evaluate(data.test_mask)\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate on Train Set\n",
        "train_precision, train_recall, train_f1, train_auc, train_accuracy = evaluate(data.train_mask)\n",
        "print(\"\\nTrain Set Performance:\")\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Train F1: {train_f1:.4f}\")\n",
        "print(f\"Train AUC: {train_auc:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Step 9: Generate Predictions for Test Set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_out = model(test_data)\n",
        "    test_probs = torch.softmax(test_out, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Index': test.index,\n",
        "    'Class': test_probs\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file generated: submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weT4dSu5EWSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}