{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "qpip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD7IxMx2FkLq",
        "outputId": "f5bb9660-ca0e-4795-9ee9-e9f3680fc70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZMKvp20Ext2",
        "outputId": "8b39eccc-012c-4dfa-e230-d49eb12f1d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (28215, 434), Test shape: (25629, 433)\n",
            "Note: The following columns are missing in the dataset: ['R_emaildomain']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-06056900af58>:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
            "<ipython-input-4-06056900af58>:81: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
            "<ipython-input-4-06056900af58>:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test['hour'] = ((test['TransactionDT'] // 3600) % 24)\n",
            "<ipython-input-4-06056900af58>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train['LogTransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
            "<ipython-input-4-06056900af58>:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['LogTransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
            "<ipython-input-4-06056900af58>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test['LogTransactionAmt'] = np.log1p(test['TransactionAmt'])\n",
            "<ipython-input-4-06056900af58>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['P_emaildomain'] = train['P_emaildomain'].apply(group_email)\n",
            "<ipython-input-4-06056900af58>:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:118: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:120: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
            "<ipython-input-4-06056900af58>:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:118: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:120: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
            "<ipython-input-4-06056900af58>:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:118: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:120: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
            "<ipython-input-4-06056900af58>:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:118: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "<ipython-input-4-06056900af58>:120: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
            "<ipython-input-4-06056900af58>:125: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['DeviceType'] = train['DeviceInfo'].str.split('/', expand=True)[0]\n",
            "<ipython-input-4-06056900af58>:127: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['DeviceType'] = train['DeviceType'].fillna('unknown')\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n",
            "<ipython-input-4-06056900af58>:148: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col] = le.transform(train[col])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6996\n",
            "Epoch 10, Loss: 0.3987\n",
            "Epoch 20, Loss: 0.2547\n",
            "Epoch 30, Loss: 0.2233\n",
            "Epoch 40, Loss: 0.2066\n",
            "Epoch 50, Loss: 0.1909\n",
            "Epoch 60, Loss: 0.1827\n",
            "Epoch 70, Loss: 0.1772\n",
            "Epoch 80, Loss: 0.1678\n",
            "Epoch 90, Loss: 0.1635\n",
            "\n",
            "Test Set Performance:\n",
            "Test Precision: 0.9048\n",
            "Test Recall: 0.7518\n",
            "Test F1: 0.8212\n",
            "Test AUC: 0.8900\n",
            "Test Accuracy: 0.8379\n",
            "\n",
            "Train Set Performance:\n",
            "Train Precision: 0.8981\n",
            "Train Recall: 0.7547\n",
            "Train F1: 0.8201\n",
            "Train AUC: 0.8895\n",
            "Train Accuracy: 0.8344\n",
            "Warning: Test predictions length (25629) does not match sample_submission length (506691)\n",
            "Submission file generated: submission.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Step 1: Load the Data\n",
        "train_identity = pd.read_csv(r'/content/train_identity.csv')\n",
        "train_transaction = pd.read_csv(r'/content/train_transaction.csv')\n",
        "test_identity = pd.read_csv(r'/content/test_identity.csv')\n",
        "test_transaction = pd.read_csv(r'/content/test_transaction.csv')\n",
        "sample_submission = pd.read_csv(r'/content/sample_submission.csv')\n",
        "\n",
        "# Step 1.1: Fix column names in test_identity\n",
        "test_identity.columns = [col.replace('id-', 'id_') for col in test_identity.columns]\n",
        "\n",
        "# Step 1.2: Combine the datasets\n",
        "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
        "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n",
        "\n",
        "# Store original test TransactionIDs for submission alignment\n",
        "test_transaction_ids = test['TransactionID'].copy()\n",
        "\n",
        "# Clean up memory\n",
        "del train_identity, train_transaction, test_identity, test_transaction\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# Step 2: Enhanced Preprocessing\n",
        "# 2.1 Remove features with high missing values (>80%)\n",
        "missing_percent = train.isnull().mean()\n",
        "high_missing_cols = missing_percent[missing_percent > 0.8].index.tolist()\n",
        "train.drop(columns=high_missing_cols, inplace=True)\n",
        "test.drop(columns=high_missing_cols, inplace=True)\n",
        "\n",
        "# 2.2 Define potential categorical columns\n",
        "potential_categorical_cols = ['ProductCD', 'card4', 'card5', 'card6', 'P_emaildomain', 'R_emaildomain']\n",
        "\n",
        "# Verify which categorical columns exist\n",
        "categorical_cols = [col for col in potential_categorical_cols if col in train.columns]\n",
        "missing_cols = [col for col in potential_categorical_cols if col not in train.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Note: The following columns are missing in the dataset: {missing_cols}\")\n",
        "\n",
        "# 2.3 Handle missing values\n",
        "numerical_cols = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "if 'isFraud' in numerical_cols:\n",
        "    numerical_cols.remove('isFraud')\n",
        "if 'TransactionID' in numerical_cols:\n",
        "    numerical_cols.remove('TransactionID')\n",
        "\n",
        "for col in numerical_cols:\n",
        "    train[col] = train[col].fillna(train[col].median())\n",
        "    test[col] = test[col].fillna(train[col].median())  # Use train median for test to avoid data leakage\n",
        "\n",
        "for col in categorical_cols:\n",
        "    train[col] = train[col].fillna('missing').astype(str)\n",
        "    test[col] = test[col].fillna('missing').astype(str)\n",
        "\n",
        "# 2.4 Remove outliers using IQR (only for train set)\n",
        "for col in ['TransactionAmt', 'C1', 'C2']:\n",
        "    if col in train.columns:\n",
        "        q1 = train[col].quantile(0.05)\n",
        "        q3 = train[col].quantile(0.95)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]\n",
        "\n",
        "# Step 3: Enhanced Feature Engineering\n",
        "# 3.1 Time-based features\n",
        "train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
        "test['hour'] = ((test['TransactionDT'] // 3600) % 24)\n",
        "\n",
        "# 3.2 Log transform TransactionAmt\n",
        "train['LogTransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
        "test['LogTransactionAmt'] = np.log1p(test['TransactionAmt'])\n",
        "\n",
        "# 3.3 Enhanced email domain grouping\n",
        "def group_email(email):\n",
        "    if pd.isna(email) or email == 'missing':\n",
        "        return 'NoInfo'\n",
        "    email = str(email).lower()\n",
        "    if 'gmail' in email:\n",
        "        return 'Google'\n",
        "    elif 'yahoo' in email:\n",
        "        return 'Yahoo'\n",
        "    elif any(x in email for x in ['hotmail', 'outlook', 'msn', 'live']):\n",
        "        return 'Microsoft'\n",
        "    elif any(x in email for x in ['aol', 'aim']):\n",
        "        return 'AOL'\n",
        "    elif any(x in email for x in ['protonmail', 'proton']):\n",
        "        return 'Proton'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "if 'P_emaildomain' in train.columns:\n",
        "    train['P_emaildomain'] = train['P_emaildomain'].apply(group_email)\n",
        "    test['P_emaildomain'] = test['P_emaildomain'].apply(group_email)\n",
        "\n",
        "if 'R_emaildomain' in train.columns:\n",
        "    train['R_emaildomain'] = train['R_emaildomain'].apply(group_email)\n",
        "    test['R_emaildomain'] = test['R_emaildomain'].apply(group_email)\n",
        "\n",
        "# 3.4 Transaction frequency features\n",
        "for col in ['card1', 'card2', 'card3', 'card5']:\n",
        "    if col in train.columns:\n",
        "        freq_map = train[col].value_counts().to_dict()\n",
        "        train[f'{col}_freq'] = train[col].map(freq_map)\n",
        "        test[f'{col}_freq'] = test[col].map(freq_map)\n",
        "        train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
        "        test[f'{col}_freq'] = test[f'{col}_freq'].fillna(0)\n",
        "\n",
        "# 3.5 Device info features\n",
        "if 'DeviceInfo' in train.columns:\n",
        "    train['DeviceType'] = train['DeviceInfo'].str.split('/', expand=True)[0]\n",
        "    test['DeviceType'] = test['DeviceInfo'].str.split('/', expand=True)[0]\n",
        "    train['DeviceType'] = train['DeviceType'].fillna('unknown')\n",
        "    test['DeviceType'] = test['DeviceType'].fillna('unknown')\n",
        "\n",
        "# Step 4: Prepare Data for GNN\n",
        "# 4.1 Define feature set\n",
        "numerical_cols = ['LogTransactionAmt', 'hour'] + \\\n",
        "                [f'{col}_freq' for col in ['card1', 'card2', 'card3', 'card5'] if f'{col}_freq' in train.columns] + \\\n",
        "                [col for col in ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10',\n",
        "                               'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6',\n",
        "                               'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15'] if col in train.columns]\n",
        "\n",
        "feature_selection_categorical_cols = [col for col in ['ProductCD', 'card4', 'card5', 'card6',\n",
        "                                                    'P_emaildomain', 'R_emaildomain', 'DeviceType']\n",
        "                                    if col in train.columns]\n",
        "\n",
        "# 4.2 Encode categorical features\n",
        "label_encoders = {}\n",
        "for col in feature_selection_categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    combined = pd.concat([train[col], test[col]], axis=0)\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4.3 Combine features\n",
        "features = numerical_cols + feature_selection_categorical_cols\n",
        "features = [f for f in features if f in train.columns]\n",
        "\n",
        "X = train[features]\n",
        "y = train['isFraud']\n",
        "X_test_full = test[features]\n",
        "\n",
        "# 4.4 Scale features (StandardScaler first, then MinMaxScaler to [0, 0.7])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "# Apply MinMaxScaler to numerical features only\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 0.7))\n",
        "numerical_indices = [features.index(col) for col in numerical_cols if col in features]\n",
        "X_scaled[:, numerical_indices] = minmax_scaler.fit_transform(X_scaled[:, numerical_indices])\n",
        "X_test_scaled[:, numerical_indices] = minmax_scaler.transform(X_test_scaled[:, numerical_indices])\n",
        "\n",
        "# 4.5 Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# 4.6 Create graph structure (edges based on shared card1)\n",
        "def create_edge_index(df, card_col='card1'):\n",
        "    edge_index = []\n",
        "    node_to_idx = {tid: idx for idx, tid in enumerate(df.index)}\n",
        "    for card in df[card_col].unique():\n",
        "        if pd.notna(card):\n",
        "            indices = df[df[card_col] == card].index.tolist()\n",
        "            for i in range(len(indices)):\n",
        "                for j in range(i + 1, len(indices)):\n",
        "                    edge_index.append([node_to_idx[indices[i]], node_to_idx[indices[j]]])\n",
        "                    edge_index.append([node_to_idx[indices[j]], node_to_idx[indices[i]]])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "# Create edge_index for training data\n",
        "train_edge_index = create_edge_index(train)\n",
        "\n",
        "# Create edge_index for test data\n",
        "test_edge_index = create_edge_index(test)\n",
        "\n",
        "# 4.7 Create PyTorch Geometric Data object for training\n",
        "x = torch.tensor(X_resampled, dtype=torch.float)\n",
        "y = torch.tensor(y_resampled.values, dtype=torch.long)\n",
        "data = Data(x=x, edge_index=train_edge_index, y=y)\n",
        "\n",
        "# 4.8 Create train/val/test masks\n",
        "n_samples = len(y_resampled)\n",
        "train_idx, temp_idx = train_test_split(range(n_samples), test_size=0.3, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "train_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# 4.9 Create PyTorch Geometric Data object for test set\n",
        "test_data = Data(x=torch.tensor(X_test_scaled, dtype=torch.float), edge_index=test_edge_index)\n",
        "\n",
        "# Step 5: Define GNN Model\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 6: Train GNN Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNN(input_dim=X_resampled.shape[1], hidden_dim=64, output_dim=2).to(device)\n",
        "data = data.to(device)\n",
        "test_data = test_data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        probs = torch.softmax(out[mask], dim=1)[:, 1].cpu().numpy()\n",
        "        pred = (probs >= optimal_threshold).astype(int)\n",
        "        true = data.y[mask].cpu().numpy()\n",
        "        precision = precision_score(true, pred)\n",
        "        recall = recall_score(true, pred)\n",
        "        f1 = f1_score(true, pred)\n",
        "        auc = roc_auc_score(true, probs)\n",
        "        accuracy = accuracy_score(true, pred)\n",
        "    return precision, recall, f1, auc, accuracy\n",
        "\n",
        "# Find optimal threshold on validation set\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    val_probs = torch.softmax(out[data.val_mask], dim=1)[:, 1].cpu().numpy()\n",
        "    val_true = data.y[data.val_mask].cpu().numpy()\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(val_true, val_probs)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "# Step 7: Evaluate on Test Set\n",
        "test_precision, test_recall, test_f1, test_auc, test_accuracy = evaluate(data.test_mask)\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate on Train Set\n",
        "train_precision, train_recall, train_f1, train_auc, train_accuracy = evaluate(data.train_mask)\n",
        "print(\"\\nTrain Set Performance:\")\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Train F1: {train_f1:.4f}\")\n",
        "print(f\"Train AUC: {train_auc:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Step 9: Generate Submission\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_out = model(test_data)\n",
        "    test_probs = torch.softmax(test_out, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "# Ensure test_probs aligns with sample_submission\n",
        "if len(test_probs) != len(sample_submission):\n",
        "    print(f\"Warning: Test predictions length ({len(test_probs)}) does not match sample_submission length ({len(sample_submission)})\")\n",
        "    # Create a mapping to align predictions with TransactionIDs\n",
        "    test_pred_df = pd.DataFrame({\n",
        "        'TransactionID': test_transaction_ids,\n",
        "        'isFraud': test_probs\n",
        "    })\n",
        "    # Merge with sample_submission to ensure correct order and length\n",
        "    sample_submission = sample_submission.merge(test_pred_df, on='TransactionID', how='left', suffixes=('', '_new'))\n",
        "    sample_submission['isFraud'] = sample_submission['isFraud_new'].fillna(0)  # Fill missing predictions with 0\n",
        "    sample_submission = sample_submission.drop(columns=['isFraud_new'])\n",
        "else:\n",
        "    sample_submission['isFraud'] = test_probs\n",
        "\n",
        "sample_submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file generated: submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DNfNJSeE4wM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}