{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy7sTTSq8XaD",
        "outputId": "2475bd0f-5991-496a-fe79-2b4ad6f94db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric_temporal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwZZ0Wl8aF1",
        "outputId": "d1eb03ed-e31a-4f31-fbf8-d65799f72bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric_temporal\n",
            "  Using cached torch_geometric_temporal-0.56.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.6.0+cu124)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (3.0.12)\n",
            "Collecting torch_sparse (from torch_geometric_temporal)\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch_geometric_temporal)\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (4.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch_geometric_temporal) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse->torch_geometric_temporal) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch_geometric_temporal) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2025.6.15)\n",
            "Using cached torch_geometric_temporal-0.56.0-py3-none-any.whl (98 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch_scatter, torch_sparse\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=1d6c760c5cfa19c001037929176637aacb071bd90f2599971df2f7490b4e6ad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=1127937 sha256=6be8176722650420c9fcbad70f00e59f1938292dad3f0bd45f201ec261fbedaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "Successfully built torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, torch_sparse, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_geometric_temporal\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric_temporal-0.56.0 torch_scatter-2.1.2 torch_sparse-0.6.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix # Added confusion_matrix\n",
        "\n",
        "# --- Device Configuration ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Load the Data\n",
        "# Assuming these files are in the /content/ directory or accessible\n",
        "train_identity = pd.read_csv(r'/content/train_identity.csv')\n",
        "train_transaction = pd.read_csv(r'/content/train_transaction.csv')\n",
        "test_identity = pd.read_csv(r'/content/test_identity.csv')\n",
        "test_transaction = pd.read_csv(r'/content/test_transaction.csv')\n",
        "sample_submission = pd.read_csv(r'/content/sample_submission.csv')\n",
        "\n",
        "# Step 1.1: Fix column names in test_identity\n",
        "test_identity.columns = [col.replace('id-', 'id_') for col in test_identity.columns]\n",
        "\n",
        "# Step 1.2: Combine the datasets\n",
        "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
        "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n",
        "\n",
        "# Store original test TransactionIDs for submission alignment\n",
        "test_transaction_ids = test['TransactionID'].copy()\n",
        "\n",
        "# Clean up memory\n",
        "del train_identity, train_transaction, test_identity, test_transaction\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# Step 2: Enhanced Preprocessing\n",
        "# 2.1 Remove features with high missing values (>80%)\n",
        "missing_percent = train.isnull().mean()\n",
        "high_missing_cols = missing_percent[missing_percent > 0.8].index.tolist()\n",
        "train.drop(columns=high_missing_cols, inplace=True)\n",
        "test.drop(columns=high_missing_cols, inplace=True)\n",
        "\n",
        "# 2.2 Define potential categorical columns\n",
        "potential_categorical_cols = ['ProductCD', 'card4', 'card5', 'card6', 'P_emaildomain', 'R_emaildomain']\n",
        "\n",
        "# Verify which categorical columns exist\n",
        "categorical_cols = [col for col in potential_categorical_cols if col in train.columns]\n",
        "missing_cols = [col for col in potential_categorical_cols if col not in train.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Note: The following columns are missing in the dataset: {missing_cols}\")\n",
        "\n",
        "# 2.3 Handle missing values for initial numerical columns\n",
        "numerical_cols_initial = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "if 'isFraud' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('isFraud')\n",
        "if 'TransactionID' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('TransactionID')\n",
        "if 'TransactionDT' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('TransactionDT')\n",
        "\n",
        "for col in numerical_cols_initial:\n",
        "    train[col] = train[col].fillna(train[col].median())\n",
        "    test[col] = test[col].fillna(train[col].median())\n",
        "\n",
        "# 2.4 Remove outliers using IQR (only for train set)\n",
        "for col in ['TransactionAmt', 'C1', 'C2']:\n",
        "    if col in train.columns:\n",
        "        q1 = train[col].quantile(0.05)\n",
        "        q3 = train[col].quantile(0.95)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]\n",
        "\n",
        "# Step 3: Enhanced Feature Engineering\n",
        "# 3.1 Time-based features\n",
        "train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
        "test['hour'] = ((test['TransactionDT'] // 3600) % 24)\n",
        "\n",
        "# 3.2 Log transform TransactionAmt\n",
        "train['LogTransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
        "test['LogTransactionAmt'] = np.log1p(test['TransactionAmt'])\n",
        "\n",
        "# 3.3 Enhanced email domain grouping\n",
        "def group_email(email):\n",
        "    if pd.isna(email) or email == 'missing':\n",
        "        return 'NoInfo'\n",
        "    email = str(email).lower()\n",
        "    if 'gmail' in email:\n",
        "        return 'Google'\n",
        "    elif 'yahoo' in email:\n",
        "        return 'Yahoo'\n",
        "    elif any(x in email for x in ['hotmail', 'outlook', 'msn', 'live']):\n",
        "        return 'Microsoft'\n",
        "    elif any(x in email for x in ['aol', 'aim']):\n",
        "        return 'AOL'\n",
        "    elif any(x in email for x in ['protonmail', 'proton']):\n",
        "        return 'Proton'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "if 'P_emaildomain' in train.columns:\n",
        "    train['P_emaildomain'] = train['P_emaildomain'].apply(group_email)\n",
        "    test['P_emaildomain'] = test['P_emaildomain'].apply(group_email)\n",
        "\n",
        "if 'R_emaildomain' in train.columns:\n",
        "    train['R_emaildomain'] = train['R_emaildomain'].apply(group_email)\n",
        "    test['R_emaildomain'] = test['R_emaildomain'].apply(group_email)\n",
        "\n",
        "# 3.4 Transaction frequency features\n",
        "for col in ['card1', 'card2', 'card3', 'card5']:\n",
        "    if col in train.columns:\n",
        "        freq_map = train[col].value_counts().to_dict()\n",
        "        train[f'{col}_freq'] = train[col].map(freq_map)\n",
        "        test[f'{col}_freq'] = test[col].map(freq_map)\n",
        "        train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0) # Fill NaNs for freq features\n",
        "        test[f'{col}_freq'] = test[f'{col}_freq'].fillna(0)\n",
        "\n",
        "# 3.5 Device info features\n",
        "if 'DeviceInfo' in train.columns:\n",
        "    train['DeviceType'] = train['DeviceInfo'].str.split('/', expand=True)[0]\n",
        "    test['DeviceType'] = test['DeviceInfo'].str.split('/', expand=True)[0]\n",
        "    train['DeviceType'] = train['DeviceType'].fillna('unknown')\n",
        "    test['DeviceType'] = test['DeviceType'].fillna('unknown')\n",
        "\n",
        "\n",
        "# Step 4: Prepare Data for GNN and LSTM\n",
        "# 4.1 Define feature set\n",
        "numerical_cols = ['LogTransactionAmt', 'hour']\n",
        "\n",
        "# Add original card columns if they exist\n",
        "for col in ['card1', 'card2', 'card3', 'card5']:\n",
        "    if col in train.columns:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "# Add frequency features\n",
        "numerical_cols.extend([f'{col}_freq' for col in ['card1', 'card2', 'card3', 'card5'] if f'{col}_freq' in train.columns])\n",
        "\n",
        "# Add C and D columns\n",
        "numerical_cols.extend([col for col in ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10',\n",
        "                                     'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6',\n",
        "                                     'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15'] if col in train.columns])\n",
        "\n",
        "# Add 'V' and 'M' columns\n",
        "v_cols = [f'V{i}' for i in range(1, 340) if f'V{i}' in train.columns]\n",
        "m_cols = [f'M{i}' for i in range(1, 10) if f'M{i}' in train.columns]\n",
        "\n",
        "numerical_cols.extend(v_cols)\n",
        "\n",
        "# --- Handle 'M' columns which can be boolean or object types ---\n",
        "for col in m_cols:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].map({'T': 1, 'F': 0}).astype(float)\n",
        "        test[col] = test[col].map({'T': 1, 'F': 0}).astype(float)\n",
        "        train[col].fillna(0, inplace=True)\n",
        "        test[col].fillna(0, inplace=True)\n",
        "    numerical_cols.append(col)\n",
        "\n",
        "feature_selection_categorical_cols = [col for col in ['ProductCD', 'card4', 'card5', 'card6',\n",
        "                                                     'P_emaildomain', 'R_emaildomain', 'DeviceType']\n",
        "                                      if col in train.columns]\n",
        "\n",
        "# 4.2 Encode categorical features (Label Encoding for now, One-Hot is not strictly needed for GAN if features are scaled)\n",
        "label_encoders = {}\n",
        "for col in feature_selection_categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = train[col].fillna('missing').astype(str)\n",
        "    test[col] = test[col].fillna('missing').astype(str)\n",
        "    combined = pd.concat([train[col], test[col]], axis=0)\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4.3 Combine features\n",
        "features = numerical_cols + feature_selection_categorical_cols\n",
        "features = [f for f in features if f in train.columns] # Final check\n",
        "\n",
        "# --- CRITICAL: Fill NaNs for all selected features BEFORE Scaling ---\n",
        "print(\"Checking and filling any remaining NaNs in selected features...\")\n",
        "for col in features:\n",
        "    if train[col].isnull().any():\n",
        "        if col in numerical_cols:\n",
        "            median_val = train[col].median()\n",
        "            train[col].fillna(median_val, inplace=True)\n",
        "            test[col].fillna(median_val, inplace=True)\n",
        "        elif col in feature_selection_categorical_cols:\n",
        "            train[col].fillna(0, inplace=True) # Assuming 0 for missing categorical is ok after label encoding\n",
        "            test[col].fillna(0, inplace=True)\n",
        "        else: # Fallback for other potential types\n",
        "            if pd.api.types.is_numeric_dtype(train[col]):\n",
        "                median_val = train[col].median()\n",
        "                train[col].fillna(median_val, inplace=True)\n",
        "                test[col].fillna(median_val, inplace=True)\n",
        "            else: # Should ideally not hit this after proper handling\n",
        "                train[col].fillna('unknown_nan_val', inplace=True) # Fallback, but should be handled by previous steps\n",
        "                test[col].fillna('unknown_nan_val', inplace=True)\n",
        "\n",
        "print(f\"NaNs in train[features] before scaling: {train[features].isnull().sum().sum()}\")\n",
        "print(f\"NaNs in test[features] before scaling: {test[features].isnull().sum().sum()}\")\n",
        "\n",
        "# Sort data by TransactionDT for temporal processing\n",
        "train = train.sort_values(by='TransactionDT').reset_index(drop=True)\n",
        "test = test.sort_values(by='TransactionDT').reset_index(drop=True)\n",
        "\n",
        "X = train[features]\n",
        "y = train['isFraud']\n",
        "X_test_full = test[features]\n",
        "\n",
        "# --- L1-Regularized Logistic Regression for Feature Importance ---\n",
        "print(\"Performing L1-Regularized Logistic Regression for Feature Importance...\")\n",
        "temp_scaler = StandardScaler()\n",
        "X_temp_scaled = temp_scaler.fit_transform(X)\n",
        "logistic_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42, class_weight='balanced', max_iter=1000)\n",
        "logistic_reg.fit(X_temp_scaled, y)\n",
        "\n",
        "feature_importance = pd.DataFrame({'Feature': features, 'Coefficient': logistic_reg.coef_[0]})\n",
        "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "print(\"Top 20 Feature Importances (L1 Logistic Regression):\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "# 4.4 Scale features (StandardScaler first, then MinMaxScaler to [0, 0.7])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 0.7)) # Scale numerical features to 0-0.7 for GAN compatibility\n",
        "numerical_indices = [features.index(col) for col in numerical_cols if col in features]\n",
        "X_scaled[:, numerical_indices] = minmax_scaler.fit_transform(X_scaled[:, numerical_indices])\n",
        "X_test_scaled[:, numerical_indices] = minmax_scaler.transform(X_test_scaled[:, numerical_indices])\n",
        "\n",
        "# --- Store original shapes and indices before GAN augmentation ---\n",
        "original_num_train_nodes = X_scaled.shape[0]\n",
        "original_y = y.copy() # Keep original y for non-augmented validation/comparison\n",
        "\n",
        "# --- GAN for Data Augmentation ---\n",
        "print(\"\\n--- Training GAN for Data Augmentation ---\")\n",
        "\n",
        "# Separate fraud (minority) and non-fraud (majority) data for GAN training\n",
        "X_fraud_real = X_scaled[y == 1]\n",
        "X_non_fraud_real = X_scaled[y == 0]\n",
        "\n",
        "# Define GAN Architecture\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid() # Output values between 0 and 1, suitable for scaled data\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid() # Output probability\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# GAN Hyperparameters\n",
        "latent_dim = 100 # Dimension of the noise vector\n",
        "gan_output_dim = X_scaled.shape[1] # Number of features\n",
        "lr_gan = 0.0002\n",
        "b1 = 0.5 # Adam: decay of first order momentum of all gradients\n",
        "b2 = 0.999 # Adam: decay of second order momentum of all gradients\n",
        "n_epochs_gan = 3000 # Number of epochs for GAN training (can be quite high)\n",
        "batch_size_gan = 64\n",
        "\n",
        "generator = Generator(latent_dim, gan_output_dim).to(device)\n",
        "discriminator = Discriminator(gan_output_dim).to(device)\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
        "\n",
        "adversarial_loss = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "\n",
        "# Prepare real fraud data for GAN training\n",
        "real_fraud_data_tensor = torch.tensor(X_fraud_real, dtype=torch.float).to(device)\n",
        "\n",
        "# --- GAN Training Loop ---\n",
        "for epoch in range(n_epochs_gan):\n",
        "    # --- Train Discriminator ---\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # Real samples\n",
        "    # Randomly sample from the real fraud data\n",
        "    real_idx = torch.randint(0, real_fraud_data_tensor.size(0), (batch_size_gan,))\n",
        "    real_samples = real_fraud_data_tensor[real_idx]\n",
        "    real_labels = torch.ones(batch_size_gan, 1).to(device)\n",
        "    d_output_real = discriminator(real_samples)\n",
        "    d_loss_real = adversarial_loss(d_output_real, real_labels)\n",
        "\n",
        "    # Fake samples\n",
        "    z = torch.randn(batch_size_gan, latent_dim).to(device)\n",
        "    fake_samples = generator(z).detach() # Detach to prevent gradients from flowing to G\n",
        "    fake_labels = torch.zeros(batch_size_gan, 1).to(device)\n",
        "    d_output_fake = discriminator(fake_samples)\n",
        "    d_loss_fake = adversarial_loss(d_output_fake, fake_labels)\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    # --- Train Generator ---\n",
        "    optimizer_G.zero_grad()\n",
        "    z = torch.randn(batch_size_gan, latent_dim).to(device)\n",
        "    gen_samples = generator(z)\n",
        "    g_output = discriminator(gen_samples)\n",
        "    g_loss = adversarial_loss(g_output, real_labels) # Generator wants to fool D (make fake look real)\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    if (epoch + 1) % 500 == 0: # Print less frequently for high epochs\n",
        "        print(f\"GAN Epoch {epoch+1}/{n_epochs_gan}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "print(\"GAN training finished.\")\n",
        "\n",
        "# --- Generate Synthetic Fraud Data ---\n",
        "# Number of synthetic samples needed to balance the classes\n",
        "num_synthetic_samples_needed = len(X_non_fraud_real) - len(X_fraud_real)\n",
        "if num_synthetic_samples_needed < 0:\n",
        "    num_synthetic_samples_needed = 0 # In case fraud is already majority (unlikely here)\n",
        "\n",
        "print(f\"Generating {num_synthetic_samples_needed} synthetic fraud samples...\")\n",
        "\n",
        "generator.eval() # Set generator to evaluation mode\n",
        "with torch.no_grad():\n",
        "    synthetic_z = torch.randn(num_synthetic_samples_needed, latent_dim).to(device)\n",
        "    X_synthetic_fraud = generator(synthetic_z).cpu().numpy()\n",
        "\n",
        "# Combine real and synthetic fraud samples\n",
        "X_augmented_fraud = np.vstack([X_fraud_real, X_synthetic_fraud])\n",
        "y_augmented_fraud = np.ones(X_augmented_fraud.shape[0])\n",
        "\n",
        "# Combine all augmented data (original non-fraud + augmented fraud)\n",
        "X_resampled_gan = np.vstack([X_non_fraud_real, X_augmented_fraud])\n",
        "y_resampled_gan = np.hstack([np.zeros(X_non_fraud_real.shape[0]), y_augmented_fraud])\n",
        "\n",
        "# Shuffle the augmented data\n",
        "shuffled_indices = np.random.permutation(len(X_resampled_gan))\n",
        "X_resampled_gan = X_resampled_gan[shuffled_indices]\n",
        "y_resampled_gan = y_resampled_gan[shuffled_indices]\n",
        "\n",
        "print(f\"Augmented data shape (X): {X_resampled_gan.shape}\")\n",
        "print(f\"Augmented data shape (y): {y_resampled_gan.shape}\")\n",
        "print(f\"Fraud count in augmented data: {np.sum(y_resampled_gan == 1)}\")\n",
        "print(f\"Non-Fraud count in augmented data: {np.sum(y_resampled_gan == 0)}\")\n",
        "\n",
        "\n",
        "# 4.6 Create graph structure (edges based on shared card1) using augmented data\n",
        "def create_edge_index_from_series(card_series):\n",
        "    edge_index = []\n",
        "    col_to_group_by = card_series.name if card_series.name is not None else 0 # Default to 0 for unnamed series\n",
        "\n",
        "    temp_df = card_series.reset_index()\n",
        "    if col_to_group_by not in temp_df.columns:\n",
        "        # Fallback: if series name isn't a column, it's typically the first value column\n",
        "        col_to_group_by = temp_df.columns[1] # Assumes 'index' is 0, values are 1 for reset_index()\n",
        "\n",
        "    card_to_indices = temp_df.groupby(col_to_group_by)['index'].apply(list)\n",
        "\n",
        "    for card_val, indices in card_to_indices.items():\n",
        "        if pd.notna(card_val):\n",
        "            for i in range(len(indices)):\n",
        "                for j in range(i + 1, len(indices)):\n",
        "                    edge_index.append([indices[i], indices[j]])\n",
        "                    edge_index.append([indices[j], indices[i]])\n",
        "    if not edge_index:\n",
        "        print(\"Warning: No edges created. Returning empty edge_index.\")\n",
        "        return torch.empty((2, 0), dtype=torch.long)\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "if 'card1' in features:\n",
        "    card1_idx = features.index('card1')\n",
        "    # Use GAN-augmented data for training graph creation\n",
        "    temp_card1_train = pd.Series(X_resampled_gan[:, card1_idx], name='card1_values')\n",
        "    temp_card1_test = pd.Series(X_test_scaled[:, card1_idx], name='card1_values') # Test set remains unchanged\n",
        "else:\n",
        "    raise ValueError(\"card1 not found in features. This should not happen after fix. Debug.\")\n",
        "\n",
        "train_edge_index = create_edge_index_from_series(temp_card1_train)\n",
        "test_edge_index = create_edge_index_from_series(temp_card1_test)\n",
        "\n",
        "# 4.7 Create PyTorch Geometric Data object for training using GAN augmented data\n",
        "x_train_gan = torch.tensor(X_resampled_gan, dtype=torch.float)\n",
        "y_train_gan = torch.tensor(y_resampled_gan, dtype=torch.long) # Use GAN augmented labels\n",
        "data = Data(x=x_train_gan, edge_index=train_edge_index, y=y_train_gan)\n",
        "\n",
        "# 4.8 Create train/val/test masks - these will now be over the GAN-augmented data\n",
        "n_samples_gan = len(y_resampled_gan)\n",
        "# Stratify by the new, balanced y_resampled_gan\n",
        "train_idx, temp_idx = train_test_split(range(n_samples_gan), test_size=0.3, random_state=42, stratify=y_resampled_gan)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, stratify=y_resampled_gan[temp_idx])\n",
        "\n",
        "train_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# 4.9 Create PyTorch Geometric Data object for test set (original, not augmented)\n",
        "test_data = Data(x=torch.tensor(X_test_scaled, dtype=torch.float), edge_index=test_edge_index)\n",
        "\n",
        "\n",
        "# Step 5: Define Temporal GNN Model with LSTM\n",
        "class TemporalGCNLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_gnn_dim, hidden_lstm_dim, output_dim, sequence_length, dropout_rate=0.7):\n",
        "        super(TemporalGCNLSTM, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.hidden_gnn_dim = hidden_gnn_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.conv1 = GCNConv(input_dim, hidden_gnn_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_gnn_dim)\n",
        "        self.conv2 = GCNConv(hidden_gnn_dim, hidden_gnn_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_gnn_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_gnn_dim, hidden_lstm_dim, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_lstm_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_size=1):\n",
        "        x_gnn = self.conv1(x, edge_index)\n",
        "        x_gnn = self.bn1(x_gnn)\n",
        "        x_gnn = F.relu(x_gnn)\n",
        "        x_gnn = F.dropout(x_gnn, p=self.dropout_rate, training=self.training)\n",
        "        x_gnn = self.conv2(x_gnn, edge_index)\n",
        "        x_gnn = self.bn2(x_gnn)\n",
        "        x_gnn = F.relu(x_gnn)\n",
        "        x_gnn = F.dropout(x_gnn, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
        "        # Here, we treat the entire graph as a single sequence for simplicity\n",
        "        x_lstm_input = x_gnn.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x_lstm_input)\n",
        "\n",
        "        out = self.fc(lstm_out.squeeze(0)) # Remove batch dimension for final linear layer\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "# Step 6: Train Temporal GNN Model\n",
        "input_dim = X_resampled_gan.shape[1] # Use augmented data input dim\n",
        "hidden_gnn_dim = 96\n",
        "hidden_lstm_dim = 96\n",
        "output_dim = 2\n",
        "sequence_length = len(X_resampled_gan) # Use GAN augmented data length\n",
        "dropout_rate = 0.7\n",
        "\n",
        "model = TemporalGCNLSTM(input_dim=input_dim,\n",
        "                        hidden_gnn_dim=hidden_gnn_dim,\n",
        "                        hidden_lstm_dim=hidden_lstm_dim,\n",
        "                        output_dim=output_dim,\n",
        "                        sequence_length=sequence_length,\n",
        "                        dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "data = data.to(device) # Move augmented data to device\n",
        "test_data = test_data.to(device) # Move original test data to device\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "# Class weights for NLLLoss based on the new, balanced dataset\n",
        "# Since GAN aims to balance classes, weights might be closer to [1.0, 1.0] if perfectly balanced\n",
        "# Still good to calculate based on actual counts in the GAN-augmented training data\n",
        "class_weights = torch.tensor([1.0, (y_train_gan == 0).sum() / (y_train_gan == 1).sum()], dtype=torch.float).to(device)\n",
        "criterion = nn.NLLLoss(weight=class_weights)\n",
        "\n",
        "def train_model():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Modified evaluate_model to handle external test set without true labels\n",
        "def evaluate_model(mask_or_none, data_obj, optimal_threshold=None, is_original_test_set=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data_obj.x, data_obj.edge_index)\n",
        "\n",
        "        if is_original_test_set:\n",
        "            # For the *true* test set (submission data), we only return probabilities.\n",
        "            # Metrics cannot be computed as true labels are unknown.\n",
        "            probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
        "            return probs # Return probabilities directly for submission\n",
        "        else:\n",
        "            # For train/validation subsets within the *augmented* data\n",
        "            probs = torch.softmax(out[mask_or_none], dim=1)[:, 1].cpu().numpy()\n",
        "            true = data_obj.y[mask_or_none].cpu().numpy()\n",
        "\n",
        "            if optimal_threshold is None:\n",
        "                # This branch is for finding the optimal threshold on the validation set.\n",
        "                precision_curve, recall_curve, thresholds = precision_recall_curve(true, probs)\n",
        "                f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)\n",
        "\n",
        "                if len(f1_scores) > 0 and len(thresholds) > 0:\n",
        "                    optimal_idx = np.argmax(f1_scores)\n",
        "                    optimal_threshold_found = thresholds[optimal_idx]\n",
        "                else:\n",
        "                    optimal_threshold_found = 0.5 # Default threshold if no valid f1 scores (e.g., all 0s or 1s)\n",
        "                return optimal_threshold_found, probs, true # Return optimal threshold and raw probabilities/true labels\n",
        "            else:\n",
        "                # This branch is for computing metrics using a given threshold.\n",
        "                pred = (probs >= optimal_threshold).astype(int)\n",
        "                precision = precision_score(true, pred, zero_division=0)\n",
        "                recall = recall_score(true, pred, zero_division=0)\n",
        "                f1 = f1_score(true, pred, zero_division=0)\n",
        "                auc = roc_auc_score(true, probs)\n",
        "                accuracy = accuracy_score(true, pred)\n",
        "\n",
        "                # Calculate Sensitivity and Specificity separately\n",
        "                tn, fp, fn, tp = confusion_matrix(true, pred).ravel()\n",
        "\n",
        "                # Add a small epsilon to denominators to prevent division by zero\n",
        "                sensitivity = tp / (tp + fn + 1e-9) # True Positive Rate (Recall)\n",
        "                specificity = tn / (tn + fp + 1e-9) # True Negative Rate\n",
        "\n",
        "                g_mean_sensitivity = np.sqrt(sensitivity * specificity)\n",
        "\n",
        "                return precision, recall, f1, auc, accuracy, sensitivity, specificity, g_mean_sensitivity\n",
        "\n",
        "# --- Early Stopping Implementation ---\n",
        "patience = 30\n",
        "best_val_f1 = -1\n",
        "epochs_no_improve = 0\n",
        "max_epochs = 400\n",
        "\n",
        "print(\"\\nTraining TGNN+LSTM with GAN-augmented data and early stopping...\")\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_loss = train_model()\n",
        "\n",
        "    # Evaluate validation set (part of augmented data) to find best threshold\n",
        "    current_optimal_threshold, val_probs_epoch, val_true_epoch = evaluate_model(\n",
        "        data.val_mask, data, optimal_threshold=None, is_original_test_set=False\n",
        "    )\n",
        "\n",
        "    # Calculate F1 score for early stopping\n",
        "    precision_curve, recall_curve, thresholds = precision_recall_curve(val_true_epoch, val_probs_epoch)\n",
        "    f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)\n",
        "    current_val_f1 = np.max(f1_scores) if len(f1_scores) > 0 else 0.0\n",
        "\n",
        "    if current_val_f1 > best_val_f1:\n",
        "        best_val_f1 = current_val_f1\n",
        "        best_optimal_threshold = current_optimal_threshold\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epoch % 20 == 0 or epochs_no_improve == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}, Val F1: {current_val_f1:.4f}, Best Val F1: {best_val_f1:.4f}, Epochs no improve: {epochs_no_improve}')\n",
        "\n",
        "    if epochs_no_improve == patience:\n",
        "        print(f\"Early stopping at epoch {epoch} as validation F1 did not improve for {patience} epochs.\")\n",
        "        break\n",
        "\n",
        "print(f\"Optimal threshold found on validation set: {best_optimal_threshold:.4f}\")\n",
        "\n",
        "# Load the best model state for final evaluation\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "\n",
        "# Step 7: Evaluate on (GAN-augmented) Test Set (Internal Split, not the external test.csv)\n",
        "# These metrics are for the split of the augmented data used internally as a test set.\n",
        "test_precision, test_recall, test_f1, test_auc, test_accuracy, test_sensitivity, test_specificity, test_g_mean_sensitivity = evaluate_model(\n",
        "    data.test_mask, data, best_optimal_threshold, is_original_test_set=False\n",
        ")\n",
        "print(\"\\nInternal (GAN-augmented) Test Set Performance:\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall (Sensitivity): {test_recall:.4f}\")\n",
        "print(f\"Test Specificity: {test_specificity:.4f}\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test G-Mean Sensitivity: {test_g_mean_sensitivity:.4f}\")\n",
        "\n",
        "\n",
        "# Step 8: Evaluate on (GAN-augmented) Train Set (Internal Split)\n",
        "train_precision, train_recall, train_f1, train_auc, train_accuracy, train_sensitivity, train_specificity, train_g_mean_sensitivity = evaluate_model(\n",
        "    data.train_mask, data, best_optimal_threshold, is_original_test_set=False\n",
        ")\n",
        "print(\"\\nInternal (GAN-augmented) Train Set Performance:\")\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Train Recall (Sensitivity): {train_recall:.4f}\")\n",
        "print(f\"Train Specificity: {train_specificity:.4f}\")\n",
        "print(f\"Train F1: {train_f1:.4f}\")\n",
        "print(f\"Train AUC: {train_auc:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Train G-Mean Sensitivity: {train_g_mean_sensitivity:.4f}\")\n",
        "\n",
        "\n",
        "# Step 9: Generate Submission (predictions for the actual unseen test data from test.csv)\n",
        "print(\"\\nGenerating predictions for the actual unseen test data (submission file)...\")\n",
        "# For the actual test.csv data, we only generate probabilities, no true labels exist for evaluation.\n",
        "submission_probs = evaluate_model(\n",
        "    None, # No mask needed, evaluate all nodes in test_data\n",
        "    test_data,\n",
        "    best_optimal_threshold,\n",
        "    is_original_test_set=True\n",
        ")\n",
        "\n",
        "test_predictions_sorted_df = pd.DataFrame({\n",
        "    'TransactionID': test_transaction_ids, # Use the stored original test TransactionIDs\n",
        "    'isFraud': submission_probs # These are the raw probabilities\n",
        "})\n",
        "\n",
        "final_submission_df = sample_submission[['TransactionID']].merge(\n",
        "    test_predictions_sorted_df, on='TransactionID', how='left'\n",
        ")\n",
        "\n",
        "final_submission_df['isFraud'] = final_submission_df['isFraud'].fillna(0) # Ensure no NaNs in submission\n",
        "\n",
        "final_submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file generated: submission.csv\")\n",
        "print(\"Note: Performance metrics (Precision, Recall, F1, AUC, Accuracy, G-Mean Sensitivity) are not reported for the final 'test.csv' dataset as its true labels are unknown.\")"
      ],
      "metadata": {
        "id": "jDt2bbbA8b72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4ff7a2-5a67-4b72-aa3b-63e835a9fc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Train shape: (4878, 434), Test shape: (3666, 433)\n",
            "Note: The following columns are missing in the dataset: ['R_emaildomain']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2785393570.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
            "/tmp/ipython-input-5-2785393570.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test['hour'] = ((test['TransactionDT'] // 3600) % 24)\n",
            "/tmp/ipython-input-5-2785393570.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train['LogTransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
            "/tmp/ipython-input-5-2785393570.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test['LogTransactionAmt'] = np.log1p(test['TransactionAmt'])\n",
            "/tmp/ipython-input-5-2785393570.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train[f'{col}_freq'] = train[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test[f'{col}_freq'] = test[col].map(freq_map)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:162: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-5-2785393570.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[col].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking and filling any remaining NaNs in selected features...\n",
            "NaNs in train[features] before scaling: 0\n",
            "NaNs in test[features] before scaling: 0\n",
            "Performing L1-Regularized Logistic Regression for Feature Importance...\n",
            "Top 20 Feature Importances (L1 Logistic Regression):\n",
            "               Feature  Coefficient  Abs_Coefficient\n",
            "14                  C5    -1.597534         1.597534\n",
            "10                  C1     1.289573         1.289573\n",
            "114                V83     0.983129         0.983129\n",
            "169               V279    -0.914861         0.914861\n",
            "71                 V40     0.831003         0.831003\n",
            "121                V90     0.746076         0.746076\n",
            "23                 C14    -0.676903         0.676903\n",
            "176               V286     0.643280         0.643280\n",
            "170               V280     0.632459         0.632459\n",
            "77                 V46    -0.630896         0.630896\n",
            "221          ProductCD     0.624086         0.624086\n",
            "113                V82    -0.622718         0.622718\n",
            "174               V284    -0.616969         0.616969\n",
            "224              card6    -0.575455         0.575455\n",
            "0    LogTransactionAmt     0.574371         0.574371\n",
            "205               V315    -0.569143         0.569143\n",
            "13                  C4     0.563926         0.563926\n",
            "70                 V39    -0.559817         0.559817\n",
            "8           card3_freq    -0.558171         0.558171\n",
            "78                 V47     0.534845         0.534845\n",
            "\n",
            "--- Training GAN for Data Augmentation ---\n",
            "GAN Epoch 500/3000, D Loss: 0.2962, G Loss: 2.6880\n",
            "GAN Epoch 1000/3000, D Loss: 0.0685, G Loss: 3.3528\n",
            "GAN Epoch 1500/3000, D Loss: 0.3416, G Loss: 2.7394\n",
            "GAN Epoch 2000/3000, D Loss: 0.0748, G Loss: 4.5779\n",
            "GAN Epoch 2500/3000, D Loss: 0.0443, G Loss: 4.9882\n",
            "GAN Epoch 3000/3000, D Loss: 0.0095, G Loss: 5.9251\n",
            "GAN training finished.\n",
            "Generating 4360 synthetic fraud samples...\n",
            "Augmented data shape (X): (8910, 226)\n",
            "Augmented data shape (y): (8910,)\n",
            "Fraud count in augmented data: 4455\n",
            "Non-Fraud count in augmented data: 4455\n",
            "\n",
            "Training TGNN+LSTM with GAN-augmented data and early stopping...\n",
            "Epoch 1, Loss: 0.6892, Val F1: 0.8648, Best Val F1: 0.8648, Epochs no improve: 0\n",
            "Epoch 2, Loss: 0.6713, Val F1: 0.8841, Best Val F1: 0.8841, Epochs no improve: 0\n",
            "Epoch 3, Loss: 0.6543, Val F1: 0.8975, Best Val F1: 0.8975, Epochs no improve: 0\n",
            "Epoch 4, Loss: 0.6375, Val F1: 0.9080, Best Val F1: 0.9080, Epochs no improve: 0\n",
            "Epoch 5, Loss: 0.6206, Val F1: 0.9177, Best Val F1: 0.9177, Epochs no improve: 0\n",
            "Epoch 6, Loss: 0.6046, Val F1: 0.9303, Best Val F1: 0.9303, Epochs no improve: 0\n",
            "Epoch 7, Loss: 0.5856, Val F1: 0.9420, Best Val F1: 0.9420, Epochs no improve: 0\n",
            "Epoch 8, Loss: 0.5634, Val F1: 0.9532, Best Val F1: 0.9532, Epochs no improve: 0\n",
            "Epoch 9, Loss: 0.5425, Val F1: 0.9612, Best Val F1: 0.9612, Epochs no improve: 0\n",
            "Epoch 10, Loss: 0.5183, Val F1: 0.9691, Best Val F1: 0.9691, Epochs no improve: 0\n",
            "Epoch 11, Loss: 0.4936, Val F1: 0.9752, Best Val F1: 0.9752, Epochs no improve: 0\n",
            "Epoch 12, Loss: 0.4674, Val F1: 0.9790, Best Val F1: 0.9790, Epochs no improve: 0\n",
            "Epoch 13, Loss: 0.4368, Val F1: 0.9827, Best Val F1: 0.9827, Epochs no improve: 0\n",
            "Epoch 14, Loss: 0.4079, Val F1: 0.9842, Best Val F1: 0.9842, Epochs no improve: 0\n",
            "Epoch 15, Loss: 0.3789, Val F1: 0.9857, Best Val F1: 0.9857, Epochs no improve: 0\n",
            "Epoch 16, Loss: 0.3476, Val F1: 0.9864, Best Val F1: 0.9864, Epochs no improve: 0\n",
            "Epoch 17, Loss: 0.3172, Val F1: 0.9879, Best Val F1: 0.9879, Epochs no improve: 0\n",
            "Epoch 18, Loss: 0.2892, Val F1: 0.9902, Best Val F1: 0.9902, Epochs no improve: 0\n",
            "Epoch 20, Loss: 0.2307, Val F1: 0.9902, Best Val F1: 0.9902, Epochs no improve: 2\n",
            "Epoch 40, Loss: 0.0555, Val F1: 0.9902, Best Val F1: 0.9902, Epochs no improve: 22\n",
            "Epoch 46, Loss: 0.0526, Val F1: 0.9902, Best Val F1: 0.9902, Epochs no improve: 0\n",
            "Epoch 47, Loss: 0.0542, Val F1: 0.9902, Best Val F1: 0.9902, Epochs no improve: 0\n",
            "Epoch 48, Loss: 0.0515, Val F1: 0.9909, Best Val F1: 0.9909, Epochs no improve: 0\n",
            "Epoch 49, Loss: 0.0513, Val F1: 0.9910, Best Val F1: 0.9910, Epochs no improve: 0\n",
            "Epoch 60, Loss: 0.0477, Val F1: 0.9902, Best Val F1: 0.9910, Epochs no improve: 11\n",
            "Early stopping at epoch 79 as validation F1 did not improve for 30 epochs.\n",
            "Optimal threshold found on validation set: 0.2513\n",
            "\n",
            "Internal (GAN-augmented) Test Set Performance:\n",
            "Test Precision: 0.9969\n",
            "Test Recall (Sensitivity): 0.9716\n",
            "Test Specificity: 0.9970\n",
            "Test F1: 0.9841\n",
            "Test AUC: 0.9918\n",
            "Test Accuracy: 0.9843\n",
            "Test G-Mean Sensitivity: 0.9842\n",
            "\n",
            "Internal (GAN-augmented) Train Set Performance:\n",
            "Train Precision: 0.9974\n",
            "Train Recall (Sensitivity): 0.9840\n",
            "Train Specificity: 0.9974\n",
            "Train F1: 0.9906\n",
            "Train AUC: 0.9942\n",
            "Train Accuracy: 0.9907\n",
            "Train G-Mean Sensitivity: 0.9907\n",
            "\n",
            "Generating predictions for the actual unseen test data (submission file)...\n",
            "Submission file generated: submission.csv\n",
            "Note: Performance metrics (Precision, Recall, F1, AUC, Accuracy, G-Mean Sensitivity) are not reported for the final 'test.csv' dataset as its true labels are unknown.\n"
          ]
        }
      ]
    }
  ]
}