{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQoKrbKCPCBl",
        "outputId": "32cad7b3-d249-433b-8571-f8af24531210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric_temporal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h58AMG9PHGK",
        "outputId": "8bd6eba0-7562-4c88-8a0f-e2b8b842cf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric_temporal\n",
            "  Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.6.0+cu124)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (3.0.12)\n",
            "Collecting torch_sparse (from torch_geometric_temporal)\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch_geometric_temporal)\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch_geometric_temporal) (3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (4.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torch_geometric_temporal)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch_geometric_temporal) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch_geometric_temporal) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch_geometric_temporal) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse->torch_geometric_temporal) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch_geometric_temporal) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2025.6.15)\n",
            "Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch_scatter, torch_sparse\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=0355ef4073adc2c80fc5aeefd8d8c6beacb3794e0531c4d9a798a91d5aa254df\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=1127937 sha256=f68c75626f05559c7e17eb1e91a5174f134c28eb34df9c1dfd5ad351d8ec5e0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "Successfully built torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, torch_sparse, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_geometric_temporal\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric_temporal-0.56.0 torch_scatter-2.1.2 torch_sparse-0.6.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "# --- Device Configuration ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Utility Function for Edge Creation (MOVE THIS BLOCK HERE) ---\n",
        "def create_edge_index_from_series(card_series):\n",
        "    edge_index = []\n",
        "    col_to_group_by = card_series.name if card_series.name is not None else 'value' # Default name\n",
        "\n",
        "    temp_df = card_series.reset_index()\n",
        "    # If `col_to_group_by` is not found, it implies the default 'value' for reset_index() series\n",
        "    if col_to_group_by not in temp_df.columns:\n",
        "        col_to_group_by = temp_df.columns[1] # Assumes 'index' is 0, values are 1 for reset_index()\n",
        "\n",
        "    card_to_indices = temp_df.groupby(col_to_group_by)['index'].apply(list)\n",
        "\n",
        "    for card_val, indices in card_to_indices.items():\n",
        "        if pd.notna(card_val):\n",
        "            # Only create edges if there are at least two transactions with the same value\n",
        "            if len(indices) > 1:\n",
        "                for i in range(len(indices)):\n",
        "                    for j in range(i + 1, len(indices)):\n",
        "                        # Add undirected edges\n",
        "                        edge_index.append([indices[i], indices[j]])\n",
        "                        edge_index.append([indices[j], indices[i]])\n",
        "    if not edge_index:\n",
        "        print(f\"Warning: No edges created for grouping by '{col_to_group_by}'. Returning empty edge_index.\")\n",
        "        return torch.empty((2, 0), dtype=torch.long)\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "\n",
        "# Step 1: Load the Data - MODIFIED FOR creditcard.csv\n",
        "# Assuming 'creditcard.csv' is in the /content/ directory or accessible\n",
        "\n",
        "data_df = None # Initialize data_df to None\n",
        "\n",
        "try:\n",
        "    data_df = pd.read_csv(r'/content/creditcard.csv')\n",
        "    print(\"Successfully loaded 'creditcard.csv'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'creditcard.csv' not found. Please ensure the file is in the '/content/' directory.\")\n",
        "    print(\"You might need to upload it to Colab, or mount Google Drive and adjust the path.\")\n",
        "    raise # Re-raise the exception to stop execution gracefully.\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the CSV: {e}\")\n",
        "    raise # Re-raise any other exceptions\n",
        "\n",
        "# Now, we are sure data_df is loaded if we reach here\n",
        "# Rename 'Class' to 'isFraud' for consistency with the rest of your code\n",
        "if 'Class' in data_df.columns:\n",
        "    data_df.rename(columns={'Class': 'isFraud'}, inplace=True)\n",
        "    print(\"Renamed 'Class' column to 'isFraud'.\")\n",
        "else:\n",
        "    print(\"Warning: 'Class' column (expected target) not found. Please ensure the target column is named 'isFraud' or 'Class'.\")\n",
        "\n",
        "\n",
        "# We need a unique identifier for the test set. 'Time' might not be unique if transactions\n",
        "# occur at the exact same second. Let's create a 'TransactionID' column based on the index\n",
        "# if it doesn't exist, to ensure a stable merge for the submission.\n",
        "if 'TransactionID' not in data_df.columns:\n",
        "    data_df['TransactionID'] = data_df.index\n",
        "    print(\"Created 'TransactionID' column from DataFrame index.\")\n",
        "\n",
        "# To simulate train/test split like the original problem, we'll sort by 'Time'\n",
        "# and then split. The original problem used 'TransactionDT' for time.\n",
        "# We'll use the 'Time' column from 'creditcard.csv' for sorting.\n",
        "if 'Time' in data_df.columns:\n",
        "    data_df = data_df.sort_values(by='Time').reset_index(drop=True)\n",
        "    print(\"Sorted data by 'Time' column.\")\n",
        "else:\n",
        "    print(\"Warning: 'Time' column not found. Data will not be sorted by time before splitting.\")\n",
        "\n",
        "\n",
        "# Define the split point for train and test (e.g., 80% for train, 20% for test)\n",
        "train_split_ratio = 0.8\n",
        "split_point = int(len(data_df) * train_split_ratio)\n",
        "\n",
        "train = data_df.iloc[:split_point].copy()\n",
        "test = data_df.iloc[split_point:].copy()\n",
        "\n",
        "# Store original test TransactionIDs for submission alignment\n",
        "test_transaction_ids = test['TransactionID'].copy()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# Step 2: Enhanced Preprocessing\n",
        "missing_percent = train.isnull().mean()\n",
        "high_missing_cols = missing_percent[missing_percent > 0.8].index.tolist()\n",
        "if high_missing_cols:\n",
        "    train.drop(columns=high_missing_cols, inplace=True)\n",
        "    test.drop(columns=high_missing_cols, inplace=True)\n",
        "    print(f\"Removed columns with >80% missing values: {high_missing_cols}\")\n",
        "else:\n",
        "    print(\"No columns with >80% missing values found.\")\n",
        "\n",
        "potential_categorical_cols = []\n",
        "categorical_cols = [col for col in potential_categorical_cols if col in train.columns]\n",
        "missing_cols = [col for col in potential_categorical_cols if col not in train.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Note: The following expected categorical columns are missing in the dataset: {missing_cols}\")\n",
        "if categorical_cols:\n",
        "    print(f\"Identified categorical columns: {categorical_cols}\")\n",
        "else:\n",
        "    print(\"No predefined categorical columns found in the dataset (common for creditcard.csv).\")\n",
        "\n",
        "numerical_cols_initial = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "if 'isFraud' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('isFraud')\n",
        "if 'TransactionID' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('TransactionID')\n",
        "if 'Time' in numerical_cols_initial:\n",
        "    numerical_cols_initial.remove('Time')\n",
        "\n",
        "print(\"Filling missing numerical values with median (if any)...\")\n",
        "for col in numerical_cols_initial:\n",
        "    if train[col].isnull().any() or test[col].isnull().any():\n",
        "        median_val = train[col].median()\n",
        "        train[col] = train[col].fillna(median_val)\n",
        "        test[col] = test[col].fillna(median_val)\n",
        "        print(f\"Filled NaNs in {col} with median {median_val}.\")\n",
        "\n",
        "outlier_cols_to_check = ['Amount']\n",
        "if 'Amount' in train.columns:\n",
        "    print(\"Applying IQR-based outlier removal for 'Amount' in training data...\")\n",
        "    col = 'Amount'\n",
        "    q1 = train[col].quantile(0.05)\n",
        "    q3 = train[col].quantile(0.95)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    original_train_rows = train.shape[0]\n",
        "    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)].copy()\n",
        "    print(f\"Removed {original_train_rows - train.shape[0]} outlier rows based on 'Amount'. New train shape: {train.shape}\")\n",
        "else:\n",
        "    print(\"No 'Amount' column found for IQR outlier removal.\")\n",
        "\n",
        "\n",
        "# Step 3: Enhanced Feature Engineering\n",
        "if 'Time' in train.columns:\n",
        "    train.rename(columns={'Time': 'TransactionDT'}, inplace=True)\n",
        "    test.rename(columns={'Time': 'TransactionDT'}, inplace=True)\n",
        "    print(\"Renamed 'Time' to 'TransactionDT' for time-based features.\")\n",
        "\n",
        "    train['hour'] = ((train['TransactionDT'] // 3600) % 24)\n",
        "    test['hour'] = ((test['TransactionDT'] // 3600) % 24)\n",
        "    print(\"Created 'hour' feature from 'TransactionDT'.\")\n",
        "else:\n",
        "    print(\"No 'Time'/'TransactionDT' column found for time-based feature engineering.\")\n",
        "\n",
        "if 'Amount' in train.columns:\n",
        "    train['LogTransactionAmt'] = np.log1p(train['Amount'])\n",
        "    test['LogTransactionAmt'] = np.log1p(test['Amount'])\n",
        "    print(\"Created 'LogTransactionAmt' from 'Amount'.\")\n",
        "else:\n",
        "    print(\"No 'Amount' column found for Log transformation.\")\n",
        "\n",
        "print(\"Skipping email domain grouping (not applicable for creditcard.csv).\")\n",
        "print(\"Skipping card frequency features (not applicable for creditcard.csv with original columns).\")\n",
        "for col in ['card1', 'card2', 'card3', 'card5']:\n",
        "    if col in train.columns:\n",
        "        freq_map = train[col].value_counts().to_dict()\n",
        "        train[f'{col}_freq'] = train[col].map(freq_map)\n",
        "        test[f'{col}_freq'] = test[col].map(freq_map)\n",
        "        train[f'{col}_freq'] = train[f'{col}_freq'].fillna(0)\n",
        "        test[f'{col}_freq'] = test[f'{col}_freq'].fillna(0)\n",
        "\n",
        "print(\"Skipping device info features (not applicable for creditcard.csv).\")\n",
        "\n",
        "\n",
        "# Step 4: Prepare Data for GNN and LSTM\n",
        "numerical_cols = ['LogTransactionAmt', 'hour'] if 'LogTransactionAmt' in train.columns and 'hour' in train.columns else []\n",
        "\n",
        "v_cols = [f'V{i}' for i in range(1, 29) if f'V{i}' in train.columns]\n",
        "numerical_cols.extend(v_cols)\n",
        "\n",
        "if 'Amount' in train.columns and 'LogTransactionAmt' not in numerical_cols:\n",
        "    numerical_cols.append('Amount')\n",
        "\n",
        "m_cols = [f'M{i}' for i in range(1, 10) if f'M{i}' in train.columns]\n",
        "for col in m_cols:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].map({'T': 1, 'F': 0}).astype(float)\n",
        "        test[col] = test[col].map({'T': 1, 'F': 0}).astype(float)\n",
        "        train[col].fillna(0, inplace=True)\n",
        "        test[col].fillna(0, inplace=True)\n",
        "    numerical_cols.append(col)\n",
        "\n",
        "feature_selection_categorical_cols = []\n",
        "\n",
        "label_encoders = {}\n",
        "for col in feature_selection_categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = train[col].fillna('missing').astype(str)\n",
        "    test[col] = test[col].fillna('missing').astype(str)\n",
        "    combined = pd.concat([train[col], test[col]], axis=0)\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"Label encoded column: {col}\")\n",
        "\n",
        "\n",
        "features = numerical_cols + feature_selection_categorical_cols\n",
        "features = [f for f in features if f in train.columns]\n",
        "if not features:\n",
        "    raise ValueError(\"No features selected for training. Please check feature definitions.\")\n",
        "print(f\"Selected features for model: {features}\")\n",
        "\n",
        "\n",
        "print(\"Checking and filling any remaining NaNs in selected features...\")\n",
        "for col in features:\n",
        "    if train[col].isnull().any():\n",
        "        if col in numerical_cols:\n",
        "            median_val = train[col].median()\n",
        "            train[col].fillna(median_val, inplace=True)\n",
        "            test[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Filled remaining NaNs in {col} (numerical) with median {median_val}.\")\n",
        "        elif col in feature_selection_categorical_cols:\n",
        "            train[col].fillna(0, inplace=True)\n",
        "            test[col].fillna(0, inplace=True)\n",
        "            print(f\"Filled remaining NaNs in {col} (categorical) with 0.\")\n",
        "        else:\n",
        "            if pd.api.types.is_numeric_dtype(train[col]):\n",
        "                median_val = train[col].median()\n",
        "                train[col].fillna(median_val, inplace=True)\n",
        "                test[col].fillna(median_val, inplace=True)\n",
        "                print(f\"Filled remaining NaNs in {col} (unknown numeric) with median {median_val}.\")\n",
        "            else:\n",
        "                train[col].fillna('unknown_nan_val', inplace=True)\n",
        "                test[col].fillna('unknown_nan_val', inplace=True)\n",
        "                print(f\"Filled remaining NaNs in {col} (unknown type) with 'unknown_nan_val'.\")\n",
        "\n",
        "\n",
        "print(f\"NaNs in train[features] before scaling: {train[features].isnull().sum().sum()}\")\n",
        "print(f\"NaNs in test[features] before scaling: {test[features].isnull().sum().sum()}\")\n",
        "\n",
        "X = train[features]\n",
        "y = train['isFraud']\n",
        "X_test_full = test[features]\n",
        "\n",
        "print(\"Performing L1-Regularized Logistic Regression for Feature Importance...\")\n",
        "temp_scaler = StandardScaler()\n",
        "X_temp_scaled = temp_scaler.fit_transform(X)\n",
        "logistic_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42, class_weight='balanced', max_iter=1000)\n",
        "logistic_reg.fit(X_temp_scaled, y)\n",
        "\n",
        "feature_importance = pd.DataFrame({'Feature': features, 'Coefficient': logistic_reg.coef_[0]})\n",
        "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "print(\"Top 20 Feature Importances (L1 Logistic Regression):\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 0.7))\n",
        "numerical_indices = [features.index(col) for col in numerical_cols if col in features]\n",
        "X_scaled[:, numerical_indices] = minmax_scaler.fit_transform(X_scaled[:, numerical_indices])\n",
        "X_test_scaled[:, numerical_indices] = minmax_scaler.transform(X_test_scaled[:, numerical_indices])\n",
        "\n",
        "original_num_train_nodes = X_scaled.shape[0]\n",
        "original_y = y.copy()\n",
        "\n",
        "print(\"\\n--- Training GAN for Data Augmentation ---\")\n",
        "\n",
        "X_fraud_real = X_scaled[y == 1]\n",
        "X_non_fraud_real = X_scaled[y == 0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "latent_dim = 100\n",
        "gan_output_dim = X_scaled.shape[1]\n",
        "lr_gan = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_epochs_gan = 3000\n",
        "batch_size_gan = 64\n",
        "\n",
        "generator = Generator(latent_dim, gan_output_dim).to(device)\n",
        "discriminator = Discriminator(gan_output_dim).to(device)\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
        "\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "real_fraud_data_tensor = torch.tensor(X_fraud_real, dtype=torch.float).to(device)\n",
        "\n",
        "for epoch in range(n_epochs_gan):\n",
        "    optimizer_D.zero_grad()\n",
        "    real_idx = torch.randint(0, real_fraud_data_tensor.size(0), (batch_size_gan,))\n",
        "    real_samples = real_fraud_data_tensor[real_idx]\n",
        "    real_labels = torch.ones(batch_size_gan, 1).to(device)\n",
        "    d_output_real = discriminator(real_samples)\n",
        "    d_loss_real = adversarial_loss(d_output_real, real_labels)\n",
        "\n",
        "    z = torch.randn(batch_size_gan, latent_dim).to(device)\n",
        "    fake_samples = generator(z).detach()\n",
        "    fake_labels = torch.zeros(batch_size_gan, 1).to(device)\n",
        "    d_output_fake = discriminator(fake_samples)\n",
        "    d_loss_fake = adversarial_loss(d_output_fake, fake_labels)\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    optimizer_G.zero_grad()\n",
        "    z = torch.randn(batch_size_gan, latent_dim).to(device)\n",
        "    gen_samples = generator(z)\n",
        "    g_output = discriminator(gen_samples)\n",
        "    g_loss = adversarial_loss(g_output, real_labels)\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"GAN Epoch {epoch+1}/{n_epochs_gan}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "print(\"GAN training finished.\")\n",
        "\n",
        "num_synthetic_samples_needed = len(X_non_fraud_real) - len(X_fraud_real)\n",
        "if num_synthetic_samples_needed < 0:\n",
        "    num_synthetic_samples_needed = 0\n",
        "\n",
        "print(f\"Generating {num_synthetic_samples_needed} synthetic fraud samples...\")\n",
        "\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    synthetic_z = torch.randn(num_synthetic_samples_needed, latent_dim).to(device)\n",
        "    X_synthetic_fraud = generator(synthetic_z).cpu().numpy()\n",
        "\n",
        "X_augmented_fraud = np.vstack([X_fraud_real, X_synthetic_fraud])\n",
        "y_augmented_fraud = np.ones(X_augmented_fraud.shape[0])\n",
        "\n",
        "X_resampled_gan = np.vstack([X_non_fraud_real, X_augmented_fraud])\n",
        "y_resampled_gan = np.hstack([np.zeros(X_non_fraud_real.shape[0]), y_augmented_fraud])\n",
        "\n",
        "shuffled_indices = np.random.permutation(len(X_resampled_gan))\n",
        "X_resampled_gan = X_resampled_gan[shuffled_indices]\n",
        "y_resampled_gan = y_resampled_gan[shuffled_indices]\n",
        "\n",
        "print(f\"Augmented data shape (X): {X_resampled_gan.shape}\")\n",
        "print(f\"Augmented data shape (y): {y_resampled_gan.shape}\")\n",
        "print(f\"Fraud count in augmented data: {np.sum(y_resampled_gan == 1)}\")\n",
        "print(f\"Non-Fraud count in augmented data: {np.sum(y_resampled_gan == 0)}\")\n",
        "\n",
        "\n",
        "# 4.6 Create graph structure (edges based on shared card1) using augmented data\n",
        "node_grouping_feature = None\n",
        "# Prioritize 'TransactionID' as a unique identifier if we created it.\n",
        "# Otherwise, default to 'V1' as a numerical feature.\n",
        "if 'TransactionID' in features:\n",
        "    node_grouping_feature = 'TransactionID'\n",
        "elif 'V1' in features:\n",
        "    node_grouping_feature = 'V1'\n",
        "elif 'Time' in features: # Use 'TransactionDT' if 'Time' was renamed\n",
        "    node_grouping_feature = 'TransactionDT'\n",
        "else:\n",
        "    print(\"Warning: No suitable feature for graph edge creation found. Graph might be empty or problematic.\")\n",
        "\n",
        "if node_grouping_feature:\n",
        "    print(f\"Using '{node_grouping_feature}' for graph edge creation.\")\n",
        "    grouping_idx = features.index(node_grouping_feature)\n",
        "    temp_grouping_train = pd.Series(X_resampled_gan[:, grouping_idx], name=node_grouping_feature)\n",
        "    temp_grouping_test = pd.Series(X_test_scaled[:, grouping_idx], name=node_grouping_feature)\n",
        "\n",
        "    train_edge_index = create_edge_index_from_series(temp_grouping_train)\n",
        "    test_edge_index = create_edge_index_from_series(temp_grouping_test)\n",
        "else:\n",
        "    print(\"WARNING: No suitable grouping feature for GNN edge creation. Creating an empty edge index. GNN may not function as intended.\")\n",
        "    train_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "    test_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "\n",
        "# 4.7 Create PyTorch Geometric Data object for training using GAN augmented data\n",
        "x_train_gan = torch.tensor(X_resampled_gan, dtype=torch.float)\n",
        "y_train_gan = torch.tensor(y_resampled_gan, dtype=torch.long)\n",
        "data = Data(x=x_train_gan, edge_index=train_edge_index, y=y_train_gan)\n",
        "print(f\"PyG Data object created for training. Num nodes: {data.num_nodes}, Num edges: {data.num_edges}\")\n",
        "\n",
        "\n",
        "# 4.8 Create train/val/test masks - these will now be over the GAN-augmented data\n",
        "n_samples_gan = len(y_resampled_gan)\n",
        "train_idx, temp_idx = train_test_split(range(n_samples_gan), test_size=0.3, random_state=42, stratify=y_resampled_gan)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, stratify=y_resampled_gan[temp_idx])\n",
        "\n",
        "train_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_samples_gan, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "print(f\"Train mask size: {train_mask.sum()}, Val mask size: {val_mask.sum()}, Test mask size: {test_mask.sum()}\")\n",
        "\n",
        "\n",
        "# 4.9 Create PyTorch Geometric Data object for test set (original, not augmented)\n",
        "test_data = Data(x=torch.tensor(X_test_scaled, dtype=torch.float), edge_index=test_edge_index)\n",
        "print(f\"PyG Data object created for test set. Num nodes: {test_data.num_nodes}, Num edges: {test_data.num_edges}\")\n",
        "\n",
        "\n",
        "# Step 5: Define Temporal GNN Model with LSTM\n",
        "class TemporalGCNLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_gnn_dim, hidden_lstm_dim, output_dim, sequence_length, dropout_rate=0.7):\n",
        "        super(TemporalGCNLSTM, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.hidden_gnn_dim = hidden_gnn_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.conv1 = GCNConv(input_dim, hidden_gnn_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_gnn_dim)\n",
        "        self.conv2 = GCNConv(hidden_gnn_dim, hidden_gnn_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_gnn_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_gnn_dim, hidden_lstm_dim, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_lstm_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_size=1):\n",
        "        x_gnn = self.conv1(x, edge_index)\n",
        "        x_gnn = self.bn1(x_gnn)\n",
        "        x_gnn = F.relu(x_gnn)\n",
        "        x_gnn = F.dropout(x_gnn, p=self.dropout_rate, training=self.training)\n",
        "        x_gnn = self.conv2(x_gnn, edge_index)\n",
        "        x_gnn = self.bn2(x_gnn)\n",
        "        x_gnn = F.relu(x_gnn)\n",
        "        x_gnn = F.dropout(x_gnn, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x_lstm_input = x_gnn.unsqueeze(0)\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x_lstm_input)\n",
        "\n",
        "        out = self.fc(lstm_out.squeeze(0))\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "# Step 6: Train Temporal GNN Model\n",
        "input_dim = X_resampled_gan.shape[1]\n",
        "hidden_gnn_dim = 96\n",
        "hidden_lstm_dim = 96\n",
        "output_dim = 2\n",
        "sequence_length = len(X_resampled_gan)\n",
        "dropout_rate = 0.7\n",
        "\n",
        "model = TemporalGCNLSTM(input_dim=input_dim,\n",
        "                        hidden_gnn_dim=hidden_gnn_dim,\n",
        "                        hidden_lstm_dim=hidden_lstm_dim,\n",
        "                        output_dim=output_dim,\n",
        "                        sequence_length=sequence_length,\n",
        "                        dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "data = data.to(device)\n",
        "test_data = test_data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "class_weights = torch.tensor([1.0, (y_train_gan == 0).sum() / (y_train_gan == 1).sum()], dtype=torch.float).to(device)\n",
        "criterion = nn.NLLLoss(weight=class_weights)\n",
        "print(f\"Calculated class weights for NLLLoss: {class_weights}\")\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate_model(mask_or_none, data_obj, optimal_threshold=None, is_original_test_set=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data_obj.x, data_obj.edge_index)\n",
        "\n",
        "        if is_original_test_set:\n",
        "            probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
        "            return probs\n",
        "        else:\n",
        "            probs = torch.softmax(out[mask_or_none], dim=1)[:, 1].cpu().numpy()\n",
        "            true = data_obj.y[mask_or_none].cpu().numpy()\n",
        "\n",
        "            if optimal_threshold is None:\n",
        "                precision_curve, recall_curve, thresholds = precision_recall_curve(true, probs)\n",
        "                f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)\n",
        "\n",
        "                if len(f1_scores) > 0 and len(thresholds) > 0:\n",
        "                    optimal_idx = np.argmax(f1_scores)\n",
        "                    optimal_threshold_found = thresholds[optimal_idx]\n",
        "                else:\n",
        "                    optimal_threshold_found = 0.5\n",
        "                return optimal_threshold_found, probs, true\n",
        "            else:\n",
        "                pred = (probs >= optimal_threshold).astype(int)\n",
        "                precision = precision_score(true, pred, zero_division=0)\n",
        "                recall = recall_score(true, pred, zero_division=0)\n",
        "                f1 = f1_score(true, pred, zero_division=0)\n",
        "                auc = roc_auc_score(true, probs)\n",
        "                accuracy = accuracy_score(true, pred)\n",
        "\n",
        "                tn, fp, fn, tp = confusion_matrix(true, pred).ravel()\n",
        "\n",
        "                sensitivity = tp / (tp + fn + 1e-9)\n",
        "                specificity = tn / (tn + fp + 1e-9)\n",
        "\n",
        "                g_mean_sensitivity = np.sqrt(sensitivity * specificity)\n",
        "\n",
        "                return precision, recall, f1, auc, accuracy, sensitivity, specificity, g_mean_sensitivity\n",
        "\n",
        "patience = 30\n",
        "best_val_f1 = -1\n",
        "epochs_no_improve = 0\n",
        "max_epochs = 400\n",
        "\n",
        "print(\"\\nTraining TGNN+LSTM with GAN-augmented data and early stopping...\")\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_loss = train_model()\n",
        "\n",
        "    current_optimal_threshold, val_probs_epoch, val_true_epoch = evaluate_model(\n",
        "        data.val_mask, data, optimal_threshold=None, is_original_test_set=False\n",
        "    )\n",
        "\n",
        "    precision_curve, recall_curve, thresholds = precision_recall_curve(val_true_epoch, val_probs_epoch)\n",
        "    f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)\n",
        "    current_val_f1 = np.max(f1_scores) if len(f1_scores) > 0 else 0.0\n",
        "\n",
        "    if current_val_f1 > best_val_f1:\n",
        "        best_val_f1 = current_val_f1\n",
        "        best_optimal_threshold = current_optimal_threshold\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epoch % 20 == 0 or epochs_no_improve == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}, Val F1: {current_val_f1:.4f}, Best Val F1: {best_val_f1:.4f}, Epochs no improve: {epochs_no_improve}')\n",
        "\n",
        "    if epochs_no_improve == patience:\n",
        "        print(f\"Early stopping at epoch {epoch} as validation F1 did not improve for {patience} epochs.\")\n",
        "        break\n",
        "\n",
        "print(f\"Optimal threshold found on validation set: {best_optimal_threshold:.4f}\")\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "test_precision, test_recall, test_f1, test_auc, test_accuracy, test_sensitivity, test_specificity, test_g_mean_sensitivity = evaluate_model(\n",
        "    data.test_mask, data, best_optimal_threshold, is_original_test_set=False\n",
        ")\n",
        "print(\"\\nInternal (GAN-augmented) Test Set Performance:\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall (Sensitivity): {test_recall:.4f}\")\n",
        "print(f\"Test Specificity: {test_specificity:.4f}\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test G-Mean Sensitivity: {test_g_mean_sensitivity:.4f}\")\n",
        "\n",
        "\n",
        "train_precision, train_recall, train_f1, train_auc, train_accuracy, train_sensitivity, train_specificity, train_g_mean_sensitivity = evaluate_model(\n",
        "    data.train_mask, data, best_optimal_threshold, is_original_test_set=False\n",
        ")\n",
        "print(\"\\nInternal (GAN-augmented) Train Set Performance:\")\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Train Recall (Sensitivity): {train_recall:.4f}\")\n",
        "print(f\"Train Specificity: {train_specificity:.4f}\")\n",
        "print(f\"Train F1: {train_f1:.4f}\")\n",
        "print(f\"Train AUC: {train_auc:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Train G-Mean Sensitivity: {train_g_mean_sensitivity:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nGenerating predictions for the actual unseen test data (submission file)...\")\n",
        "submission_probs = evaluate_model(\n",
        "    None,\n",
        "    test_data,\n",
        "    best_optimal_threshold,\n",
        "    is_original_test_set=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    final_submission_df = sample_submission[['TransactionID']].merge(\n",
        "        pd.DataFrame({'TransactionID': test_transaction_ids, 'isFraud': submission_probs}),\n",
        "        on='TransactionID', how='left'\n",
        "    )\n",
        "    print(\"Using provided sample_submission for merge.\")\n",
        "except NameError:\n",
        "    print(\"`sample_submission` not defined. Creating submission DataFrame directly.\")\n",
        "    final_submission_df = pd.DataFrame({\n",
        "        'TransactionID': test_transaction_ids,\n",
        "        'isFraud': submission_probs\n",
        "    })\n",
        "    final_submission_df.rename(columns={'TransactionID': 'id'}, inplace=True)\n",
        "except KeyError:\n",
        "    print(\"`sample_submission` missing 'TransactionID' column. Creating submission DataFrame directly.\")\n",
        "    final_submission_df = pd.DataFrame({\n",
        "        'TransactionID': test_transaction_ids,\n",
        "        'isFraud': submission_probs\n",
        "    })\n",
        "    final_submission_df.rename(columns={'TransactionID': 'id'}, inplace=True)\n",
        "\n",
        "\n",
        "final_submission_df['isFraud'] = final_submission_df['isFraud'].fillna(0)\n",
        "\n",
        "final_submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file generated: submission.csv\")\n",
        "print(\"Note: Performance metrics (Precision, Recall, F1, AUC, Accuracy, G-Mean Sensitivity) are not reported for the final 'test.csv' dataset as its true labels are unknown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQmra_EPRFF",
        "outputId": "6997ccaf-6f9b-4f48-b984-51886e6bef31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Successfully loaded 'creditcard.csv'.\n",
            "Renamed 'Class' column to 'isFraud'.\n",
            "Created 'TransactionID' column from DataFrame index.\n",
            "Sorted data by 'Time' column.\n",
            "Train shape: (12748, 32), Test shape: (3188, 32)\n",
            "No columns with >80% missing values found.\n",
            "No predefined categorical columns found in the dataset (common for creditcard.csv).\n",
            "Filling missing numerical values with median (if any)...\n",
            "Filled NaNs in V23 with median -0.044410388486525496.\n",
            "Filled NaNs in V24 with median 0.0689371724290234.\n",
            "Filled NaNs in V25 with median 0.1526519343664955.\n",
            "Filled NaNs in V26 with median -0.02015846823294905.\n",
            "Filled NaNs in V27 with median -0.0008983923168723.\n",
            "Filled NaNs in V28 with median 0.015861526966713953.\n",
            "Filled NaNs in Amount with median 15.364999999999998.\n",
            "Applying IQR-based outlier removal for 'Amount' in training data...\n",
            "Removed 159 outlier rows based on 'Amount'. New train shape: (12589, 32)\n",
            "Renamed 'Time' to 'TransactionDT' for time-based features.\n",
            "Created 'hour' feature from 'TransactionDT'.\n",
            "Created 'LogTransactionAmt' from 'Amount'.\n",
            "Skipping email domain grouping (not applicable for creditcard.csv).\n",
            "Skipping card frequency features (not applicable for creditcard.csv with original columns).\n",
            "Skipping device info features (not applicable for creditcard.csv).\n",
            "Selected features for model: ['LogTransactionAmt', 'hour', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
            "Checking and filling any remaining NaNs in selected features...\n",
            "NaNs in train[features] before scaling: 0\n",
            "NaNs in test[features] before scaling: 0\n",
            "Performing L1-Regularized Logistic Regression for Feature Importance...\n",
            "Top 20 Feature Importances (L1 Logistic Regression):\n",
            "              Feature  Coefficient  Abs_Coefficient\n",
            "5                  V4     2.198944         2.198944\n",
            "15                V14    -1.124032         1.124032\n",
            "13                V12    -1.121862         1.121862\n",
            "20                V19    -1.116699         1.116699\n",
            "11                V10    -0.852978         0.852978\n",
            "8                  V7     0.774938         0.774938\n",
            "28                V27    -0.726943         0.726943\n",
            "12                V11    -0.674539         0.674539\n",
            "0   LogTransactionAmt    -0.614045         0.614045\n",
            "24                V23     0.609583         0.609583\n",
            "1                hour     0.575790         0.575790\n",
            "7                  V6    -0.493312         0.493312\n",
            "21                V20     0.487155         0.487155\n",
            "17                V16     0.473720         0.473720\n",
            "4                  V3     0.463918         0.463918\n",
            "18                V17    -0.419999         0.419999\n",
            "22                V21     0.325971         0.325971\n",
            "14                V13    -0.295343         0.295343\n",
            "10                 V9    -0.259576         0.259576\n",
            "23                V22    -0.199360         0.199360\n",
            "\n",
            "--- Training GAN for Data Augmentation ---\n",
            "GAN Epoch 500/3000, D Loss: 1.0899, G Loss: 0.9491\n",
            "GAN Epoch 1000/3000, D Loss: 1.5345, G Loss: 0.7039\n",
            "GAN Epoch 1500/3000, D Loss: 1.3735, G Loss: 0.7985\n",
            "GAN Epoch 2000/3000, D Loss: 1.0455, G Loss: 1.0207\n",
            "GAN Epoch 2500/3000, D Loss: 1.0596, G Loss: 0.9667\n",
            "GAN Epoch 3000/3000, D Loss: 1.0559, G Loss: 1.0150\n",
            "GAN training finished.\n",
            "Generating 12483 synthetic fraud samples...\n",
            "Augmented data shape (X): (25072, 30)\n",
            "Augmented data shape (y): (25072,)\n",
            "Fraud count in augmented data: 12536\n",
            "Non-Fraud count in augmented data: 12536\n",
            "Using 'V1' for graph edge creation.\n",
            "PyG Data object created for training. Num nodes: 25072, Num edges: 3778\n",
            "Train mask size: 17550, Val mask size: 3761, Test mask size: 3761\n",
            "PyG Data object created for test set. Num nodes: 3188, Num edges: 522\n",
            "Calculated class weights for NLLLoss: tensor([1., 1.])\n",
            "\n",
            "Training TGNN+LSTM with GAN-augmented data and early stopping...\n",
            "Epoch 1, Loss: 0.6938, Val F1: 0.6670, Best Val F1: 0.6670, Epochs no improve: 0\n",
            "Epoch 2, Loss: 0.6816, Val F1: 0.7350, Best Val F1: 0.7350, Epochs no improve: 0\n",
            "Epoch 3, Loss: 0.6701, Val F1: 0.7964, Best Val F1: 0.7964, Epochs no improve: 0\n",
            "Epoch 4, Loss: 0.6588, Val F1: 0.8330, Best Val F1: 0.8330, Epochs no improve: 0\n",
            "Epoch 5, Loss: 0.6460, Val F1: 0.8553, Best Val F1: 0.8553, Epochs no improve: 0\n",
            "Epoch 6, Loss: 0.6342, Val F1: 0.8705, Best Val F1: 0.8705, Epochs no improve: 0\n",
            "Epoch 7, Loss: 0.6199, Val F1: 0.8803, Best Val F1: 0.8803, Epochs no improve: 0\n",
            "Epoch 8, Loss: 0.6030, Val F1: 0.8886, Best Val F1: 0.8886, Epochs no improve: 0\n",
            "Epoch 9, Loss: 0.5867, Val F1: 0.8960, Best Val F1: 0.8960, Epochs no improve: 0\n",
            "Epoch 10, Loss: 0.5688, Val F1: 0.9042, Best Val F1: 0.9042, Epochs no improve: 0\n",
            "Epoch 11, Loss: 0.5487, Val F1: 0.9111, Best Val F1: 0.9111, Epochs no improve: 0\n",
            "Epoch 12, Loss: 0.5258, Val F1: 0.9184, Best Val F1: 0.9184, Epochs no improve: 0\n",
            "Epoch 13, Loss: 0.5034, Val F1: 0.9271, Best Val F1: 0.9271, Epochs no improve: 0\n",
            "Epoch 14, Loss: 0.4801, Val F1: 0.9349, Best Val F1: 0.9349, Epochs no improve: 0\n",
            "Epoch 15, Loss: 0.4527, Val F1: 0.9430, Best Val F1: 0.9430, Epochs no improve: 0\n",
            "Epoch 16, Loss: 0.4268, Val F1: 0.9501, Best Val F1: 0.9501, Epochs no improve: 0\n",
            "Epoch 17, Loss: 0.4007, Val F1: 0.9582, Best Val F1: 0.9582, Epochs no improve: 0\n",
            "Epoch 18, Loss: 0.3683, Val F1: 0.9686, Best Val F1: 0.9686, Epochs no improve: 0\n",
            "Epoch 19, Loss: 0.3403, Val F1: 0.9752, Best Val F1: 0.9752, Epochs no improve: 0\n",
            "Epoch 20, Loss: 0.3113, Val F1: 0.9810, Best Val F1: 0.9810, Epochs no improve: 0\n",
            "Epoch 21, Loss: 0.2843, Val F1: 0.9863, Best Val F1: 0.9863, Epochs no improve: 0\n",
            "Epoch 22, Loss: 0.2562, Val F1: 0.9901, Best Val F1: 0.9901, Epochs no improve: 0\n",
            "Epoch 23, Loss: 0.2309, Val F1: 0.9933, Best Val F1: 0.9933, Epochs no improve: 0\n",
            "Epoch 24, Loss: 0.2040, Val F1: 0.9957, Best Val F1: 0.9957, Epochs no improve: 0\n",
            "Epoch 25, Loss: 0.1814, Val F1: 0.9963, Best Val F1: 0.9963, Epochs no improve: 0\n",
            "Epoch 26, Loss: 0.1637, Val F1: 0.9968, Best Val F1: 0.9968, Epochs no improve: 0\n",
            "Epoch 27, Loss: 0.1467, Val F1: 0.9971, Best Val F1: 0.9971, Epochs no improve: 0\n",
            "Epoch 28, Loss: 0.1283, Val F1: 0.9971, Best Val F1: 0.9971, Epochs no improve: 0\n",
            "Epoch 29, Loss: 0.1163, Val F1: 0.9973, Best Val F1: 0.9973, Epochs no improve: 0\n",
            "Epoch 30, Loss: 0.1030, Val F1: 0.9976, Best Val F1: 0.9976, Epochs no improve: 0\n",
            "Epoch 31, Loss: 0.0929, Val F1: 0.9976, Best Val F1: 0.9976, Epochs no improve: 0\n",
            "Epoch 32, Loss: 0.0832, Val F1: 0.9979, Best Val F1: 0.9979, Epochs no improve: 0\n",
            "Epoch 33, Loss: 0.0741, Val F1: 0.9979, Best Val F1: 0.9979, Epochs no improve: 0\n",
            "Epoch 34, Loss: 0.0658, Val F1: 0.9981, Best Val F1: 0.9981, Epochs no improve: 0\n",
            "Epoch 35, Loss: 0.0595, Val F1: 0.9984, Best Val F1: 0.9984, Epochs no improve: 0\n",
            "Epoch 36, Loss: 0.0539, Val F1: 0.9984, Best Val F1: 0.9984, Epochs no improve: 0\n",
            "Epoch 37, Loss: 0.0490, Val F1: 0.9987, Best Val F1: 0.9987, Epochs no improve: 0\n",
            "Epoch 38, Loss: 0.0449, Val F1: 0.9989, Best Val F1: 0.9989, Epochs no improve: 0\n",
            "Epoch 40, Loss: 0.0364, Val F1: 0.9989, Best Val F1: 0.9989, Epochs no improve: 0\n",
            "Epoch 42, Loss: 0.0319, Val F1: 0.9992, Best Val F1: 0.9992, Epochs no improve: 0\n",
            "Epoch 55, Loss: 0.0146, Val F1: 0.9995, Best Val F1: 0.9995, Epochs no improve: 0\n",
            "Epoch 58, Loss: 0.0147, Val F1: 0.9997, Best Val F1: 0.9997, Epochs no improve: 0\n",
            "Epoch 60, Loss: 0.0135, Val F1: 0.9997, Best Val F1: 0.9997, Epochs no improve: 2\n",
            "Epoch 66, Loss: 0.0119, Val F1: 1.0000, Best Val F1: 1.0000, Epochs no improve: 0\n",
            "Epoch 80, Loss: 0.0096, Val F1: 1.0000, Best Val F1: 1.0000, Epochs no improve: 14\n",
            "Early stopping at epoch 96 as validation F1 did not improve for 30 epochs.\n",
            "Optimal threshold found on validation set: 0.7818\n",
            "\n",
            "Internal (GAN-augmented) Test Set Performance:\n",
            "Test Precision: 0.9995\n",
            "Test Recall (Sensitivity): 0.9989\n",
            "Test Specificity: 0.9995\n",
            "Test F1: 0.9992\n",
            "Test AUC: 0.9999\n",
            "Test Accuracy: 0.9992\n",
            "Test G-Mean Sensitivity: 0.9992\n",
            "\n",
            "Internal (GAN-augmented) Train Set Performance:\n",
            "Train Precision: 0.9990\n",
            "Train Recall (Sensitivity): 0.9990\n",
            "Train Specificity: 0.9990\n",
            "Train F1: 0.9990\n",
            "Train AUC: 0.9996\n",
            "Train Accuracy: 0.9990\n",
            "Train G-Mean Sensitivity: 0.9990\n",
            "\n",
            "Generating predictions for the actual unseen test data (submission file)...\n",
            "`sample_submission` not defined. Creating submission DataFrame directly.\n",
            "Submission file generated: submission.csv\n",
            "Note: Performance metrics (Precision, Recall, F1, AUC, Accuracy, G-Mean Sensitivity) are not reported for the final 'test.csv' dataset as its true labels are unknown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tgq7w8yOWLLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}