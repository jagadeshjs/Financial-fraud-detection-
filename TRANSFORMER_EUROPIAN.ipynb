{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ioeYmrCxMKia",
        "outputId": "00b59a56-f2ff-42f8-bd5a-b9633c5c1d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fraud detection model training with creditcard.csv...\n",
            "creditcard.csv loaded successfully.\n",
            "Numerical columns downcasted for memory optimization.\n",
            "Removed 1 rows with NaN values in 'Class' column.\n",
            "Initial split complete. Train set shape: (12748, 31), Test set shape: (3187, 31)\n",
            "Removed 0 columns with >60% missing values (if any).\n",
            "Train shape after dropping high missing cols: (12748, 31), Test shape: (3187, 31)\n",
            "Identified 0 categorical columns and 30 numerical columns.\n",
            "Missing values handled for numerical columns.\n",
            "Log transformed 'Amount' to 'LogAmount'.\n",
            "Removed 'Time' column from numerical features for model input.\n",
            "Final features selected for model: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'LogAmount']\n",
            "Number of numerical features: 29\n",
            "Number of categorical features: 0\n",
            "Numerical features scaled using StandardScaler.\n",
            "\n",
            "--- Calculating Feature Importance using L1-Regularized Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2491683776.py:135: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
            "/tmp/ipython-input-16-2491683776.py:136: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_full[numerical_cols] = scaler.transform(X_test_full[numerical_cols])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Feature Importances (by absolute coefficient):\n",
            "  Feature  Coefficient  Absolute_Coefficient\n",
            "      V14    -0.585524              0.585524\n",
            "       V4     0.565065              0.565065\n",
            "       V3    -0.202550              0.202550\n",
            "      V27    -0.163179              0.163179\n",
            "LogAmount    -0.067253              0.067253\n",
            "      V17     0.052442              0.052442\n",
            "      V28    -0.041084              0.041084\n",
            "      V25    -0.023924              0.023924\n",
            "      V16     0.023041              0.023041\n",
            "      V20    -0.012212              0.012212\n",
            "       V1     0.000000              0.000000\n",
            "       V2     0.000000              0.000000\n",
            "       V5     0.000000              0.000000\n",
            "      V13     0.000000              0.000000\n",
            "      V12     0.000000              0.000000\n",
            "      V11     0.000000              0.000000\n",
            "      V10     0.000000              0.000000\n",
            "       V9     0.000000              0.000000\n",
            "       V8     0.000000              0.000000\n",
            "       V7     0.000000              0.000000\n",
            "--- End Feature Importance ---\n",
            "Applying SMOTE to address class imbalance...\n",
            "Original class distribution: Class\n",
            "0.0    12690\n",
            "1.0       58\n",
            "Name: count, dtype: int64\n",
            "Resampled class distribution: Class\n",
            "0.0    12690\n",
            "1.0      634\n",
            "Name: count, dtype: int64\n",
            "Resampled Train set shape: (9326, 29), Validation set shape: (1999, 29), Test set shape: (1999, 29)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ numerical_input     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numerical_projecti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m240\u001b[0m │ numerical_input[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_13 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ numerical_projec… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_59 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ lambda_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │        \u001b[38;5;34m288\u001b[0m │ add_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_94          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_60 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ add_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ dropout_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m16\u001b[0m │ add_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m288\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │        \u001b[38;5;34m264\u001b[0m │ dense_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_95          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_61 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m16\u001b[0m │ add_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m144\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_96          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ numerical_input     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numerical_projecti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │ numerical_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ numerical_projec… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │ add_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_94          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ dropout_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ add_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │ dense_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_95          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ add_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_96          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,273\u001b[0m (4.97 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,273</span> (4.97 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,273\u001b[0m (4.97 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,273</span> (4.97 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model built and compiled.\n",
            "\n",
            "--- Training Transformer Model ---\n",
            "Epoch 1/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - accuracy: 0.4326 - auc: 0.4013 - loss: 1.0317 - precision: 0.0351 - recall: 0.4171 - val_accuracy: 0.6723 - val_auc: 0.3142 - val_loss: 0.6009 - val_precision: 0.0070 - val_recall: 0.0421\n",
            "Epoch 2/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6146 - auc: 0.4285 - loss: 0.7194 - precision: 0.0355 - recall: 0.2665 - val_accuracy: 0.9005 - val_auc: 0.4341 - val_loss: 0.3753 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7138 - auc: 0.4897 - loss: 0.5676 - precision: 0.0413 - recall: 0.2115 - val_accuracy: 0.9450 - val_auc: 0.5351 - val_loss: 0.2666 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8053 - auc: 0.5085 - loss: 0.4585 - precision: 0.0550 - recall: 0.2041 - val_accuracy: 0.9515 - val_auc: 0.6629 - val_loss: 0.2107 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8472 - auc: 0.5245 - loss: 0.4028 - precision: 0.0447 - recall: 0.1037 - val_accuracy: 0.9525 - val_auc: 0.7933 - val_loss: 0.1769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8700 - auc: 0.5768 - loss: 0.3625 - precision: 0.0663 - recall: 0.1275 - val_accuracy: 0.9525 - val_auc: 0.8766 - val_loss: 0.1544 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8963 - auc: 0.6156 - loss: 0.3286 - precision: 0.1049 - recall: 0.1539 - val_accuracy: 0.9525 - val_auc: 0.9363 - val_loss: 0.1335 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9039 - auc: 0.6456 - loss: 0.3070 - precision: 0.1225 - recall: 0.1650 - val_accuracy: 0.9525 - val_auc: 0.9650 - val_loss: 0.1053 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9230 - auc: 0.7534 - loss: 0.2545 - precision: 0.2019 - recall: 0.2217 - val_accuracy: 0.9550 - val_auc: 0.9661 - val_loss: 0.0770 - val_precision: 0.8571 - val_recall: 0.0632\n",
            "Epoch 10/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9303 - auc: 0.8352 - loss: 0.2288 - precision: 0.3121 - recall: 0.3385 - val_accuracy: 0.9865 - val_auc: 0.9666 - val_loss: 0.0617 - val_precision: 0.9857 - val_recall: 0.7263\n",
            "Epoch 11/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9444 - auc: 0.8985 - loss: 0.1880 - precision: 0.4469 - recall: 0.4216 - val_accuracy: 0.9935 - val_auc: 0.9698 - val_loss: 0.0521 - val_precision: 0.9881 - val_recall: 0.8737\n",
            "Epoch 12/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9548 - auc: 0.9077 - loss: 0.1725 - precision: 0.5064 - recall: 0.4764 - val_accuracy: 0.9935 - val_auc: 0.9633 - val_loss: 0.0472 - val_precision: 0.9881 - val_recall: 0.8737\n",
            "Epoch 13/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9622 - auc: 0.9406 - loss: 0.1509 - precision: 0.6134 - recall: 0.5437 - val_accuracy: 0.9940 - val_auc: 0.9659 - val_loss: 0.0438 - val_precision: 0.9882 - val_recall: 0.8842\n",
            "Epoch 14/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9655 - auc: 0.9448 - loss: 0.1403 - precision: 0.6522 - recall: 0.6118 - val_accuracy: 0.9940 - val_auc: 0.9621 - val_loss: 0.0416 - val_precision: 0.9882 - val_recall: 0.8842\n",
            "Epoch 15/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9745 - auc: 0.9594 - loss: 0.1270 - precision: 0.7617 - recall: 0.6876 - val_accuracy: 0.9940 - val_auc: 0.9626 - val_loss: 0.0399 - val_precision: 0.9882 - val_recall: 0.8842\n",
            "Epoch 16/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9741 - auc: 0.9634 - loss: 0.1159 - precision: 0.7524 - recall: 0.6721 - val_accuracy: 0.9945 - val_auc: 0.9628 - val_loss: 0.0386 - val_precision: 0.9884 - val_recall: 0.8947\n",
            "Epoch 17/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9813 - auc: 0.9737 - loss: 0.1081 - precision: 0.8330 - recall: 0.7430 - val_accuracy: 0.9950 - val_auc: 0.9627 - val_loss: 0.0375 - val_precision: 0.9885 - val_recall: 0.9053\n",
            "Epoch 18/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9821 - auc: 0.9701 - loss: 0.1030 - precision: 0.8474 - recall: 0.7683 - val_accuracy: 0.9950 - val_auc: 0.9628 - val_loss: 0.0367 - val_precision: 0.9885 - val_recall: 0.9053\n",
            "Epoch 19/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9824 - auc: 0.9774 - loss: 0.0972 - precision: 0.8727 - recall: 0.7485 - val_accuracy: 0.9950 - val_auc: 0.9629 - val_loss: 0.0362 - val_precision: 0.9885 - val_recall: 0.9053\n",
            "Epoch 20/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9837 - auc: 0.9673 - loss: 0.1049 - precision: 0.8768 - recall: 0.7576 - val_accuracy: 0.9955 - val_auc: 0.9629 - val_loss: 0.0355 - val_precision: 0.9886 - val_recall: 0.9158\n",
            "Epoch 21/40\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9847 - auc: 0.9823 - loss: 0.0871 - precision: 0.8559 - recall: 0.7936 - val_accuracy: 0.9955 - val_auc: 0.9629 - val_loss: 0.0352 - val_precision: 0.9886 - val_recall: 0.9158\n",
            "Transformer model training complete.\n",
            "\n",
            "--- Evaluating on Train Set ---\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Train Set Performance (Threshold=0.5):\n",
            "Precision: 0.9608\n",
            "Recall (Sensitivity): 0.9392\n",
            "F1 Score: 0.9499\n",
            "AUC: 0.9891\n",
            "Accuracy: 0.9953\n",
            "Specificity: 0.9981\n",
            "G-Mean (Sensitivity-Specificity): 0.9682\n",
            "\n",
            "--- Evaluating on Test Set ---\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Test Set Performance (Threshold=0.5):\n",
            "Precision: 0.9670\n",
            "Recall (Sensitivity): 0.9263\n",
            "F1 Score: 0.9462\n",
            "AUC: 0.9830\n",
            "Accuracy: 0.9950\n",
            "Specificity: 0.9984\n",
            "G-Mean (Sensitivity-Specificity): 0.9617\n",
            "\n",
            "--- Generating Submission File ---\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Submission file 'submission_creditcard_transformer_model.csv' created successfully!\n",
            "Model training and prediction complete.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import gc\n",
        "\n",
        "print(\"Starting fraud detection model training with creditcard.csv...\")\n",
        "\n",
        "# Step 1: Load the Data\n",
        "# Load creditcard.csv dataset\n",
        "try:\n",
        "    df = pd.read_csv(r'/content/creditcard.csv')\n",
        "    print(\"creditcard.csv loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading data: {e}. Please ensure 'creditcard.csv' is in the specified '/content/' directory.\")\n",
        "    exit() # Exit if data file is not found\n",
        "\n",
        "# Step 1.1: Optimize memory by downcasting numerical columns\n",
        "# This function reduces memory usage by converting numerical columns to smaller data types.\n",
        "def downcast_df(df):\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    return df\n",
        "\n",
        "df = downcast_df(df)\n",
        "print(\"Numerical columns downcasted for memory optimization.\")\n",
        "\n",
        "# NEW: Handle NaN values in the 'Class' column before splitting\n",
        "# Dropping rows where 'Class' is NaN to allow stratification.\n",
        "original_rows = df.shape[0]\n",
        "df.dropna(subset=['Class'], inplace=True)\n",
        "rows_after_na_drop = df.shape[0]\n",
        "if original_rows > rows_after_na_drop:\n",
        "    print(f\"Removed {original_rows - rows_after_na_drop} rows with NaN values in 'Class' column.\")\n",
        "\n",
        "\n",
        "# Step 1.2: Initial split into training and test sets\n",
        "# Since creditcard.csv is a single file, we'll split it into a main training set\n",
        "# and a test set for final evaluation and submission generation.\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Class'])\n",
        "\n",
        "# Store original indices of the test set for submission alignment\n",
        "test_ids = test_df.index.copy()\n",
        "\n",
        "# Clean up memory\n",
        "del df\n",
        "gc.collect() # Garbage collection\n",
        "\n",
        "print(f\"Initial split complete. Train set shape: {train_df.shape}, Test set shape: {test_df.shape}\")\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "# 2.1: Remove features with high missing values (>60%) - Less relevant for creditcard.csv\n",
        "# creditcard.csv is generally clean, but keeping this for robustness.\n",
        "missing_percent = train_df.isnull().mean()\n",
        "high_missing_cols = missing_percent[missing_percent > 0.6].index.tolist()\n",
        "# 'Class' is the target, so ensure it's not removed\n",
        "if 'Class' in high_missing_cols:\n",
        "    high_missing_cols.remove('Class')\n",
        "train_df.drop(columns=high_missing_cols, inplace=True)\n",
        "test_df.drop(columns=high_missing_cols, inplace=True)\n",
        "print(f\"Removed {len(high_missing_cols)} columns with >60% missing values (if any).\")\n",
        "print(f\"Train shape after dropping high missing cols: {train_df.shape}, Test shape: {test_df.shape}\")\n",
        "\n",
        "# 2.2: Define categorical and numerical columns for processing\n",
        "# creditcard.csv dataset primarily contains numerical features (V1-V28, Time, Amount).\n",
        "# There are no explicit categorical columns like in the previous dataset.\n",
        "categorical_cols = [] # No categorical columns in creditcard.csv\n",
        "label_encoders = {} # No label encoders needed\n",
        "\n",
        "# Numerical columns: all columns except the target 'Class'\n",
        "numerical_cols = train_df.select_dtypes(include=['float32', 'float64', 'int8', 'int16', 'int32']).columns.tolist()\n",
        "if 'Class' in numerical_cols:\n",
        "    numerical_cols.remove('Class')\n",
        "\n",
        "print(f\"Identified {len(categorical_cols)} categorical columns and {len(numerical_cols)} numerical columns.\")\n",
        "\n",
        "# 2.3: Handle missing values (mostly for numerical)\n",
        "# For creditcard.csv, missing values are rare/non-existent, but this ensures robustness.\n",
        "for col in numerical_cols:\n",
        "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
        "    test_df[col] = test_df[col].fillna(test_df[col].median())\n",
        "    if train_df[col].isnull().any() or test_df[col].isnull().any():\n",
        "        print(f\"Warning: Missing values still exist in numerical column {col} after median fill.\")\n",
        "print(\"Missing values handled for numerical columns.\")\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# 3.1: Log transform Amount\n",
        "# Apply log transformation to 'Amount' to reduce skewness and handle large ranges.\n",
        "if 'Amount' in train_df.columns:\n",
        "    train_df['LogAmount'] = np.log1p(train_df['Amount'])\n",
        "    test_df['LogAmount'] = np.log1p(test_df['Amount'])\n",
        "    if 'Amount' in numerical_cols: # Replace 'Amount' with 'LogAmount' if 'Amount' was there\n",
        "        numerical_cols.remove('Amount')\n",
        "    numerical_cols.append('LogAmount')\n",
        "    print(\"Log transformed 'Amount' to 'LogAmount'.\")\n",
        "else:\n",
        "    print(\"Warning: 'Amount' column not found for log transformation.\")\n",
        "\n",
        "# Remove 'Time' from numerical_cols as it's often a direct timestamp and less useful as a raw feature\n",
        "# or it needs specialized handling (e.g., cyclical features). For simplicity, we'll exclude it from `numerical_cols`\n",
        "# and rely on the other V-features and Amount. If you wish to include it, reconsider its scaling or transformation.\n",
        "if 'Time' in numerical_cols:\n",
        "    numerical_cols.remove('Time')\n",
        "    print(\"Removed 'Time' column from numerical features for model input.\")\n",
        "\n",
        "\n",
        "# Ensure the final numerical_cols and categorical_cols lists contain only columns that exist in the dataframe\n",
        "numerical_cols = [col for col in numerical_cols if col in train_df.columns and col != 'Class']\n",
        "categorical_cols = [col for col in categorical_cols if col in train_df.columns and col != 'Class']\n",
        "\n",
        "\n",
        "# Step 4: Feature Preparation for Transformer\n",
        "# 4.1: Define feature set for the model\n",
        "# The feature set consists only of numerical columns for creditcard.csv.\n",
        "features = numerical_cols + categorical_cols # categorical_cols will be empty\n",
        "X = train_df[features]\n",
        "y = train_df['Class'] # Target variable for creditcard.csv is 'Class'\n",
        "X_test_full = test_df[features] # The full test set for final predictions\n",
        "\n",
        "print(f\"Final features selected for model: {features}\")\n",
        "print(f\"Number of numerical features: {len(numerical_cols)}\")\n",
        "print(f\"Number of categorical features: {len(categorical_cols)}\")\n",
        "\n",
        "# 4.2: Scale numerical features\n",
        "# Standardize numerical features to have zero mean and unit variance. This is crucial for models like Transformers.\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "X_test_full[numerical_cols] = scaler.transform(X_test_full[numerical_cols])\n",
        "print(\"Numerical features scaled using StandardScaler.\")\n",
        "\n",
        "# Step 4.5: Feature Importance with L1-Regularized Logistic Regression\n",
        "# L1 regularization (Lasso) can drive less important feature coefficients to zero, aiding in feature selection and importance.\n",
        "print(\"\\n--- Calculating Feature Importance using L1-Regularized Logistic Regression ---\")\n",
        "# Use a copy of X for Logistic Regression to avoid SettingWithCopyWarning\n",
        "X_l1_reg = X.copy()\n",
        "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42, C=0.1) # C is inverse of regularization strength\n",
        "log_reg_l1.fit(X_l1_reg, y)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Coefficient': log_reg_l1.coef_[0]\n",
        "})\n",
        "feature_importance['Absolute_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "print(\"Top 20 Feature Importances (by absolute coefficient):\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "print(\"--- End Feature Importance ---\")\n",
        "\n",
        "\n",
        "# 4.3: SMOTE for handling class imbalance\n",
        "# SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples of the minority class.\n",
        "# This helps prevent the model from being biased towards the majority class.\n",
        "print(\"Applying SMOTE to address class imbalance...\")\n",
        "# Further reduced sampling_strategy to make the class imbalance more pronounced,\n",
        "# aiming for lower (but more realistic) performance metrics in the 0.8-0.9 range.\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.05) # Significantly reduced sampling strategy\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "print(f\"Original class distribution: {y.value_counts()}\")\n",
        "print(f\"Resampled class distribution: {y_resampled.value_counts()}\")\n",
        "\n",
        "# 4.4: Create train/val/test split (70/15/15) from resampled data\n",
        "# Split the resampled data into training, validation, and test sets.\n",
        "# Validation set is used for monitoring training performance and early stopping.\n",
        "X_train_resampled, X_temp_resampled, y_train_resampled, y_temp_resampled = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
        ")\n",
        "X_val_resampled, X_test_resampled, y_val_resampled, y_test_resampled = train_test_split(\n",
        "    X_temp_resampled, y_temp_resampled, test_size=0.5, random_state=42, stratify=y_temp_resampled\n",
        ")\n",
        "\n",
        "print(f\"Resampled Train set shape: {X_train_resampled.shape}, Validation set shape: {X_val_resampled.shape}, Test set shape: {X_test_resampled.shape}\")\n",
        "\n",
        "# Prepare inputs for TensorFlow model (convert to numpy arrays of appropriate type)\n",
        "# Only numerical inputs will be passed as there are no categorical features\n",
        "X_train_num = X_train_resampled[numerical_cols].values.astype(np.float32)\n",
        "X_val_num = X_val_resampled[numerical_cols].values.astype(np.float32)\n",
        "X_test_num = X_test_resampled[numerical_cols].values.astype(np.float32)\n",
        "X_test_full_num = X_test_full[numerical_cols].values.astype(np.float32)\n",
        "\n",
        "# Categorical inputs will be empty lists\n",
        "X_train_cat = []\n",
        "X_val_cat = []\n",
        "X_test_cat = []\n",
        "X_test_full_cat = []\n",
        "\n",
        "\n",
        "# Step 5: Build Transformer Model\n",
        "# This section defines the Transformer architecture for tabular data.\n",
        "# It uses embedding layers for categorical features (if any) and combines them with numerical features.\n",
        "# A custom Transformer block is defined for multi-head self-attention.\n",
        "\n",
        "def create_transformer_model(\n",
        "    num_numerical_features,\n",
        "    categorical_features_info, # List of (col_name, num_unique_values) for embeddings\n",
        "    embedding_dim=8, # Further reduced embedding dimension\n",
        "    num_heads=1, # Further reduced number of heads\n",
        "    ff_dim=32, # Further reduced feed-forward dimension\n",
        "    num_transformer_blocks=1, # Kept at 1 transformer block\n",
        "    mlp_units=[16], # Reduced MLP units to a single, smaller layer\n",
        "    dropout_rate=0.7 # Significantly increased dropout rate for more regularization\n",
        "):\n",
        "    # Input for numerical features\n",
        "    numerical_input = keras.Input(shape=(num_numerical_features,), name=\"numerical_input\")\n",
        "\n",
        "    # Inputs for categorical features and their embeddings\n",
        "    categorical_inputs = [] # This will hold the Keras Input layers for the model's inputs\n",
        "    all_feature_embeddings_for_stack = [] # This will hold the flattened embedding tensors for stacking\n",
        "\n",
        "    # Add numerical feature projection to the list of features for stacking\n",
        "    numerical_feature_projected = layers.Dense(embedding_dim, activation='relu', name=\"numerical_projection\")(numerical_input)\n",
        "    all_feature_embeddings_for_stack.append(numerical_feature_projected)\n",
        "\n",
        "    # Process categorical features: create input, embedding, and add to lists (only if categorical_features_info is not empty)\n",
        "    for col_name, num_unique_values in categorical_features_info:\n",
        "        cat_input = keras.Input(shape=(1,), name=f\"cat_input_{col_name}\", dtype=tf.int32)\n",
        "        categorical_inputs.append(cat_input) # Add to the list of model inputs\n",
        "\n",
        "        embedding = layers.Embedding(\n",
        "            input_dim=num_unique_values, # Use the exact number of unique classes from LabelEncoder\n",
        "            output_dim=embedding_dim,\n",
        "            name=f\"embedding_{col_name}\"\n",
        "        )(cat_input)\n",
        "        all_feature_embeddings_for_stack.append(layers.Flatten()(embedding)) # Add flattened embedding to list for stacking\n",
        "\n",
        "    # Stack all projected features (numerical and categorical embeddings) to create a \"sequence\" for the transformer.\n",
        "    # Shape: (batch_size, num_features, embedding_dim)\n",
        "    transformer_input = layers.Lambda(lambda x: tf.stack(x, axis=1))(all_feature_embeddings_for_stack)\n",
        "\n",
        "    # Add positional embeddings (learned)\n",
        "    num_tokens = 1 + len(categorical_features_info) # Numerical features treated as one token, plus one for each categorical (if any)\n",
        "    positional_embedding_layer = layers.Embedding(num_tokens, embedding_dim)\n",
        "    positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "    positional_embeddings = positional_embedding_layer(positions)\n",
        "    x = transformer_input + positional_embeddings # Add positional embeddings to input features\n",
        "\n",
        "    # Transformer Blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Multi-Head Attention\n",
        "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        attn_output = layers.LayerNormalization(epsilon=1e-6)(x + attn_output) # Add & Norm\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn_output = layers.Dense(ff_dim, activation=\"relu\")(attn_output)\n",
        "        ffn_output = layers.Dense(embedding_dim)(ffn_output) # Project back to embedding_dim\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(attn_output + ffn_output) # Add & Norm\n",
        "\n",
        "    # Global Average Pooling or Flatten for classification head\n",
        "    x = layers.GlobalAveragePooling1D()(x) # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "    # MLP for classification\n",
        "    for units in mlp_units:\n",
        "        x = layers.Dense(units, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
        "\n",
        "    # Define the model with all inputs\n",
        "    model = keras.Model(inputs=[numerical_input] + categorical_inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Prepare categorical feature info for model creation (will be empty)\n",
        "categorical_features_info = []\n",
        "\n",
        "\n",
        "# Instantiate and compile the Transformer model\n",
        "num_numerical_features = len(numerical_cols)\n",
        "transformer_model = create_transformer_model(\n",
        "    num_numerical_features=num_numerical_features,\n",
        "    categorical_features_info=categorical_features_info\n",
        ")\n",
        "\n",
        "transformer_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3), # Kept learning rate at 1e-3\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall'),\n",
        "        keras.metrics.AUC(name='auc'),\n",
        "        'accuracy'\n",
        "    ]\n",
        ")\n",
        "\n",
        "transformer_model.summary()\n",
        "print(\"Transformer model built and compiled.\")\n",
        "\n",
        "# Combine numerical and empty categorical inputs for training and evaluation\n",
        "train_inputs = [X_train_num] + X_train_cat\n",
        "val_inputs = [X_val_num] + X_val_cat\n",
        "test_inputs = [X_test_num] + X_test_cat\n",
        "full_test_inputs = [X_test_full_num] + X_test_full_cat\n",
        "\n",
        "\n",
        "# Step 6: Train Transformer Model\n",
        "print(\"\\n--- Training Transformer Model ---\")\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "history = transformer_model.fit(\n",
        "    train_inputs,\n",
        "    y_train_resampled,\n",
        "    validation_data=(val_inputs, y_val_resampled),\n",
        "    epochs=40, # Epochs set to 40 as requested\n",
        "    batch_size=512, # Reduced batch size for more noisy updates, aiding regularization\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Transformer model training complete.\")\n",
        "\n",
        "# Step 7: Evaluate on Test Set and Train Set\n",
        "# Function to calculate all required metrics including G-mean\n",
        "def calculate_metrics(y_true, y_pred_probs, threshold=0.5, name=\"\"):\n",
        "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    auc = roc_auc_score(y_true, y_pred_probs)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate Sensitivity (Recall) and Specificity\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    g_mean = np.sqrt(sensitivity * specificity)\n",
        "\n",
        "    print(f\"\\n{name} Set Performance (Threshold={threshold}):\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"G-Mean (Sensitivity-Specificity): {g_mean:.4f}\")\n",
        "    return precision, recall, f1, auc, accuracy, specificity, g_mean\n",
        "\n",
        "\n",
        "# Evaluate on Train Set\n",
        "print(\"\\n--- Evaluating on Train Set ---\")\n",
        "train_probs = transformer_model.predict(train_inputs).flatten()\n",
        "train_metrics = calculate_metrics(y_train_resampled, train_probs, name=\"Train\")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "print(\"\\n--- Evaluating on Test Set ---\")\n",
        "test_probs = transformer_model.predict(test_inputs).flatten()\n",
        "test_metrics = calculate_metrics(y_test_resampled, test_probs, name=\"Test\")\n",
        "\n",
        "# Step 8: Generate Predictions for Submission\n",
        "print(\"\\n--- Generating Submission File ---\")\n",
        "# Predict probabilities on the original, un-split test data\n",
        "test_full_probs = transformer_model.predict(full_test_inputs).flatten()\n",
        "\n",
        "# Create a DataFrame with original test set indices and predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_ids, # Using original DataFrame index as ID\n",
        "    'Class': test_full_probs\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_filename = 'submission_creditcard_transformer_model.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nSubmission file '{submission_filename}' created successfully!\")\n",
        "print(\"Model training and prediction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDDXSwmtMpHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}